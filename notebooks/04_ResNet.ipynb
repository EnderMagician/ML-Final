{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# TabularResNet for TDE Classification\n",
                "\n",
                "This notebook implements a ResNet-style neural network for tabular data.\n",
                "\n",
                "**Architecture:**\n",
                "- Input Layer: Linear (input_dim -> hidden_dim)\n",
                "- ResNet Blocks: 1-3 blocks with skip connections\n",
                "- Output Layer: Linear (hidden_dim -> 1)\n",
                "\n",
                "**Training:**\n",
                "- Optuna hyperparameter tuning with MedianPruner\n",
                "- BCEWithLogitsLoss with pos_weight for class imbalance\n",
                "- Bagging: 5 models per fold with different seeds\n",
                "- Generates individual model submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b22ea651",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "print(torch.__version__)           # Should say something like '2.5.1+cu124'\n",
                "print(torch.cuda.is_available())   # Should return True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "import optuna\n",
                "from sklearn.metrics import precision_recall_curve\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "N_OPTUNA_TRIALS = 30\n",
                "N_EPOCHS_TUNING = 50\n",
                "N_EPOCHS_FINAL = 100\n",
                "N_BAGS = 5  # Models per fold for bagging\n",
                "BATCH_SIZE = 64\n",
                "MODEL_NAME = 'resnet'\n",
                "\n",
                "# Paths\n",
                "DATA_DIR = os.path.join('..', 'data', 'processed')\n",
                "MODEL_DIR = os.path.join('..', 'models')\n",
                "SUBMISSION_DIR = os.path.join('..', 'submissions')\n",
                "TRAIN_PATH = os.path.join(DATA_DIR, 'train_processed_nn_further.parquet')\n",
                "TEST_PATH = os.path.join(DATA_DIR, 'test_processed_nn_further.parquet')\n",
                "FOLDS_PATH = os.path.join(DATA_DIR, 'train_folds.csv')\n",
                "\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "os.makedirs(SUBMISSION_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "print(\"Loading NN-processed data...\")\n",
                "train = pd.read_parquet(TRAIN_PATH)\n",
                "test = pd.read_parquet(TEST_PATH)\n",
                "folds = pd.read_csv(FOLDS_PATH)\n",
                "\n",
                "# Merge folds\n",
                "train = train.merge(folds[['object_id', 'kfold']], on='object_id', how='left')\n",
                "\n",
                "print(f\"Train shape: {train.shape}\")\n",
                "print(f\"Test shape: {test.shape}\")\n",
                "print(f\"Class distribution: {train['target'].value_counts().to_dict()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "prepare_features",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features\n",
                "drop_cols = ['object_id', 'target', 'split', 'SpecType', 'kfold']\n",
                "feature_cols = [c for c in train.columns if c not in drop_cols]\n",
                "\n",
                "X = train[feature_cols].values.astype(np.float32)\n",
                "y = train['target'].values.astype(np.float32)\n",
                "kfold = train['kfold'].values\n",
                "\n",
                "X_test = test[feature_cols].values.astype(np.float32)\n",
                "object_ids_test = test['object_id']\n",
                "\n",
                "# Calculate pos_weight for imbalance (~19.5)\n",
                "pos_weight = (y == 0).sum() / (y == 1).sum()\n",
                "print(f\"Feature count: {len(feature_cols)}\")\n",
                "print(f\"pos_weight: {pos_weight:.2f}\")\n",
                "\n",
                "INPUT_DIM = len(feature_cols)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model_definition",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ResNetBlock(nn.Module):\n",
                "    \"\"\"A single ResNet block for tabular data.\"\"\"\n",
                "    def __init__(self, hidden_dim, dropout):\n",
                "        super().__init__()\n",
                "        self.block = nn.Sequential(\n",
                "            nn.Linear(hidden_dim, hidden_dim),\n",
                "            nn.BatchNorm1d(hidden_dim),\n",
                "            nn.PReLU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(hidden_dim, hidden_dim),\n",
                "            nn.BatchNorm1d(hidden_dim),\n",
                "            nn.PReLU(),\n",
                "            nn.Dropout(dropout)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return x + self.block(x)  # Skip connection\n",
                "\n",
                "\n",
                "class TabularResNet(nn.Module):\n",
                "    \"\"\"ResNet-style architecture for tabular data.\"\"\"\n",
                "    def __init__(self, input_dim, hidden_dim, n_blocks, dropout):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Input layer\n",
                "        self.input_layer = nn.Sequential(\n",
                "            nn.Linear(input_dim, hidden_dim),\n",
                "            nn.BatchNorm1d(hidden_dim),\n",
                "            nn.PReLU(),\n",
                "            nn.Dropout(dropout)\n",
                "        )\n",
                "        \n",
                "        # ResNet blocks\n",
                "        self.blocks = nn.ModuleList([\n",
                "            ResNetBlock(hidden_dim, dropout) for _ in range(n_blocks)\n",
                "        ])\n",
                "        \n",
                "        # Output layer\n",
                "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.input_layer(x)\n",
                "        for block in self.blocks:\n",
                "            x = block(x)\n",
                "        return self.output_layer(x).squeeze(-1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "training_functions",
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, criterion, optimizer):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    for X_batch, y_batch in loader:\n",
                "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(X_batch)\n",
                "        loss = criterion(outputs, y_batch)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        total_loss += loss.item()\n",
                "    return total_loss / len(loader)\n",
                "\n",
                "\n",
                "def evaluate(model, X_val, y_val):\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        X_val_t = torch.FloatTensor(X_val).to(device)\n",
                "        preds = torch.sigmoid(model(X_val_t)).cpu().numpy()\n",
                "    \n",
                "    prec, rec, _ = precision_recall_curve(y_val, preds)\n",
                "    f1 = 2 * (prec * rec) / (prec + rec + 1e-9)\n",
                "    return np.max(f1), preds"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "optuna_objective",
            "metadata": {},
            "outputs": [],
            "source": [
                "def objective(trial):\n",
                "    # Hyperparameters to tune\n",
                "    hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256, 512])\n",
                "    n_blocks = trial.suggest_int('n_blocks', 1, 3)\n",
                "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
                "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
                "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
                "    \n",
                "    # Global OOF Evaluation: Initialize OOF predictions array\n",
                "    oof_preds = np.zeros(len(y))\n",
                "    \n",
                "    for fold in range(5):\n",
                "        train_idx = kfold != fold\n",
                "        val_idx = kfold == fold\n",
                "        \n",
                "        X_tr, X_val = X[train_idx], X[val_idx]\n",
                "        y_tr, y_val = y[train_idx], y[val_idx]\n",
                "        \n",
                "        # Create DataLoader\n",
                "        train_dataset = TensorDataset(\n",
                "            torch.FloatTensor(X_tr),\n",
                "            torch.FloatTensor(y_tr)\n",
                "        )\n",
                "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "        \n",
                "        # Create model\n",
                "        model = TabularResNet(INPUT_DIM, hidden_dim, n_blocks, dropout).to(device)\n",
                "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
                "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
                "        \n",
                "        # Train\n",
                "        for epoch in range(N_EPOCHS_TUNING):\n",
                "            train_epoch(model, train_loader, criterion, optimizer)\n",
                "        \n",
                "        # Store predictions globally (do NOT calculate F1 here)\n",
                "        _, preds = evaluate(model, X_val, y_val)\n",
                "        oof_preds[val_idx] = preds\n",
                "        \n",
                "        # Pruning - use intermediate F1 for pruning only\n",
                "        prec_tmp, rec_tmp, _ = precision_recall_curve(y[val_idx], preds)\n",
                "        f1_tmp = 2 * (prec_tmp * rec_tmp) / (prec_tmp + rec_tmp + 1e-9)\n",
                "        trial.report(np.max(f1_tmp), fold)\n",
                "        if trial.should_prune():\n",
                "            raise optuna.TrialPruned()\n",
                "    \n",
                "    # Calculate Global Metric (outside loop)\n",
                "    # Optimize threshold on the full dataset\n",
                "    prec, rec, thresholds = precision_recall_curve(y, oof_preds)\n",
                "    f1_scores = 2 * (prec * rec) / (prec + rec + 1e-9)\n",
                "    best_f1 = np.max(f1_scores)\n",
                "    \n",
                "    return best_f1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run_optuna",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Optuna optimization\n",
                "print(f\"Running Optuna with {N_OPTUNA_TRIALS} trials...\")\n",
                "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2)\n",
                "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
                "study.optimize(objective, n_trials=N_OPTUNA_TRIALS, show_progress_bar=True)\n",
                "\n",
                "print(f\"\\nBest F1 Score: {study.best_value:.4f}\")\n",
                "print(f\"Best params: {study.best_params}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_final",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train final models with bagging\n",
                "print(f\"\\nTraining final models with bagging ({N_BAGS} models per fold)...\")\n",
                "\n",
                "best_params = study.best_params\n",
                "oof_preds = np.zeros(len(y))\n",
                "test_preds = np.zeros(len(X_test))\n",
                "\n",
                "for fold in range(5):\n",
                "    train_idx = kfold != fold\n",
                "    val_idx = kfold == fold\n",
                "    \n",
                "    X_tr, X_val = X[train_idx], X[val_idx]\n",
                "    y_tr, y_val = y[train_idx], y[val_idx]\n",
                "    \n",
                "    fold_val_preds = np.zeros(len(X_val))\n",
                "    fold_test_preds = np.zeros(len(X_test))\n",
                "    \n",
                "    for bag in range(N_BAGS):\n",
                "        seed = 15 + bag * 100\n",
                "        torch.manual_seed(seed)\n",
                "        np.random.seed(seed)\n",
                "        \n",
                "        # Create DataLoader\n",
                "        train_dataset = TensorDataset(\n",
                "            torch.FloatTensor(X_tr),\n",
                "            torch.FloatTensor(y_tr)\n",
                "        )\n",
                "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "        \n",
                "        # Create model\n",
                "        model = TabularResNet(\n",
                "            INPUT_DIM,\n",
                "            best_params['hidden_dim'],\n",
                "            best_params['n_blocks'],\n",
                "            best_params['dropout']\n",
                "        ).to(device)\n",
                "        \n",
                "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
                "        optimizer = torch.optim.AdamW(\n",
                "            model.parameters(),\n",
                "            lr=best_params['lr'],\n",
                "            weight_decay=best_params['weight_decay']\n",
                "        )\n",
                "        \n",
                "        # Train\n",
                "        for epoch in range(N_EPOCHS_FINAL):\n",
                "            train_epoch(model, train_loader, criterion, optimizer)\n",
                "        \n",
                "        # Predict\n",
                "        _, val_pred = evaluate(model, X_val, y_val)\n",
                "        fold_val_preds += val_pred / N_BAGS\n",
                "        \n",
                "        model.eval()\n",
                "        with torch.no_grad():\n",
                "            X_test_t = torch.FloatTensor(X_test).to(device)\n",
                "            test_pred = torch.sigmoid(model(X_test_t)).cpu().numpy()\n",
                "        fold_test_preds += test_pred / N_BAGS\n",
                "    \n",
                "    oof_preds[val_idx] = fold_val_preds\n",
                "    test_preds += fold_test_preds / 5\n",
                "    \n",
                "    print(f\"Fold {fold} complete.\")\n",
                "\n",
                "# Calculate final OOF F1\n",
                "prec, rec, thresh = precision_recall_curve(y, oof_preds)\n",
                "f1 = 2 * (prec[:-1] * rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-9)\n",
                "best_thresh = thresh[np.argmax(f1)]\n",
                "print(f\"\\nOOF F1 Score: {np.max(f1):.4f} at threshold {best_thresh:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save_predictions",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save predictions\n",
                "oof_df = pd.DataFrame({\n",
                "    'object_id': train['object_id'],\n",
                "    'target': y,\n",
                "    f'pred_{MODEL_NAME}': oof_preds\n",
                "})\n",
                "oof_df.to_csv(os.path.join(MODEL_DIR, f'oof_{MODEL_NAME}.csv'), index=False)\n",
                "\n",
                "test_df = pd.DataFrame({\n",
                "    'object_id': object_ids_test,\n",
                "    f'pred_{MODEL_NAME}': test_preds\n",
                "})\n",
                "test_df.to_csv(os.path.join(MODEL_DIR, f'preds_{MODEL_NAME}.csv'), index=False)\n",
                "\n",
                "print(f\"\\nSaved OOF predictions to: models/oof_{MODEL_NAME}.csv\")\n",
                "print(f\"Saved test predictions to: models/preds_{MODEL_NAME}.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "create_submission",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create submission file for this model\n",
                "# Apply optimal threshold from OOF\n",
                "test_binary = (test_preds >= best_thresh).astype(int)\n",
                "\n",
                "# Create submission dataframe\n",
                "submission = pd.DataFrame({\n",
                "    'object_id': object_ids_test,\n",
                "    'target': test_binary\n",
                "})\n",
                "\n",
                "submission_path = os.path.join(SUBMISSION_DIR, f'submission_{MODEL_NAME}.csv')\n",
                "submission.to_csv(submission_path, index=False)\n",
                "\n",
                "print(f\"\\n=== {MODEL_NAME.upper()} Submission ===\")\n",
                "print(f\"Threshold: {best_thresh:.4f}\")\n",
                "print(f\"Prediction distribution: {np.bincount(test_binary)}\")\n",
                "print(f\"Positive rate: {test_binary.mean():.4f}\")\n",
                "print(f\"Saved to: {submission_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "MLFinal",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}