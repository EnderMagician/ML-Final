{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":117462,"databundleVersionId":14093316,"sourceType":"competition"},{"sourceId":14175064,"sourceType":"datasetVersion","datasetId":9035629}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport gc\nimport warnings\nfrom tqdm.auto import tqdm\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, WhiteKernel\nimport xgboost as xgb\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# ==========================================\n# 1. CONFIGURATION\n# ==========================================\nclass Config:\n    # Adjust this path to the actual root of your Kaggle input\n    INPUT_ROOT = '/kaggle/input/raw-mallorn' \n    OUTPUT_FILE_TRAIN = 'mallorn_train_features.parquet'\n    OUTPUT_FILE_TEST = 'mallorn_test_features.parquet'\n    \n    # Augmentation Settings\n    N_AUGMENTATIONS = 2  # Number of synthetic TDEs to generate per real TDE\n    SEED = 42\n    PROCESS_GP = True    # Set to False to skip GP (saves time during debugging)\n    \n    # LSST Extinction Coefficients (Approximate R_lambda for u, g, r, i, z, y)\n    # A_lambda = R_lambda * E(B-V)\n    EXTINCTION_COEFFS = {\n        'u': 4.81, 'g': 3.64, 'r': 2.70, \n        'i': 2.06, 'z': 1.58, 'y': 1.31\n    }\nclass GPConfig:\n    \"\"\"Gaussian Process Hyperparameters & LSST Filter Definitions\"\"\"\n    \n    # LSST Central Wavelengths (in nanometers)\n    # These map the categorical 'Filter' column to a continuous physical dimension.\n    WAVELENGTHS = {\n        'u': 368.0,\n        'g': 480.0,\n        'r': 622.0,\n        'i': 754.0,\n        'z': 868.0,\n        'y': 973.0\n    }\n    \n    # 2D Kernel Length Scales: [Time (days), Wavelength (nm)]\n    # - Time: ~50.0 days (covers typical duration of SNe and TDEs)\n    # - Wavelength: ~2000.0 nm (Large scale implies bands are highly correlated,\n    #   which helps the GP \"borrow\" info from well-sampled bands to fill gaps in sparse bands)\n    LENGTH_SCALE = [50.0, 2000.0] \n    \n    # Optimization Bounds\n    LENGTH_SCALE_BOUNDS = [(1.0, 200.0), (100.0, 10000.0)]\n    \n    # Noise Level (white noise variance to handle Flux_err)\n    NOISE_LEVEL = 1.0 \n    NOISE_LEVEL_BOUNDS = (1e-5, 1e5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T14:46:09.276494Z","iopub.execute_input":"2025-12-16T14:46:09.276826Z","iopub.status.idle":"2025-12-16T14:46:09.285600Z","shell.execute_reply.started":"2025-12-16T14:46:09.276803Z","shell.execute_reply":"2025-12-16T14:46:09.284092Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ==========================================\n# 2. DATA LOADING & MERGING\n# ==========================================\ndef load_and_merge_data(meta_df, is_train=True):\n    \"\"\"\n    Iterates through the 'split' column in metadata to load distributed lightcurves.\n    \"\"\"\n    all_lightcurves = []\n    \n    # Get unique splits (e.g., 'Split_1', 'Split_2')\n    unique_splits = meta_df['split'].unique()\n    \n    print(f\"üìÇ Loading lightcurves from {len(unique_splits)} splits...\")\n    \n    for split_name in tqdm(unique_splits):\n        # Construct path: Root / Split_Name / [train/test]_full_lightcurves.csv\n        # Note: Adjust path construction based on exact folder names in input\n        file_prefix = \"train\" if is_train else \"test\"\n        \n        # Try finding the file in the directory structure\n        # Assumption: Folder is named exactly as in 'split' col, or needs slight adjustment\n        path = f\"{Config.INPUT_ROOT}/{split_name}/{file_prefix}_full_lightcurves.csv\"\n        \n        if not os.path.exists(path):\n            # Fallback: sometimes folder names differ slightly (e.g. 'Split 01' vs 'Split_01')\n            # This is a robust search\n            possible_paths = glob.glob(f\"{Config.INPUT_ROOT}/*{split_name}*/*{file_prefix}_full_lightcurves.csv\")\n            if possible_paths:\n                path = possible_paths[0]\n            else:\n                print(f\"‚ö†Ô∏è Warning: Could not find file for split {split_name}\")\n                continue\n                \n        # Load only necessary cols to save memory\n        chunk = pd.read_csv(path, usecols=['object_id', 'Time (MJD)', 'Flux', 'Flux_err', 'Filter'])\n        all_lightcurves.append(chunk)\n        \n    if not all_lightcurves:\n        raise ValueError(\"No lightcurve data loaded!\")\n        \n    full_lc = pd.concat(all_lightcurves, ignore_index=True)\n    \n    return full_lc\n\n# Load Metadata (Logs)\nprint(\"üìñ Loading Metadata (Log files)...\")\ntrain_log = pd.read_csv(f'{Config.INPUT_ROOT}/train_log.csv')\ntest_log = pd.read_csv(f'{Config.INPUT_ROOT}/test_log.csv')\n\n# Load Lightcurves\nprint(\"üìñ Loading Train Lightcurves...\")\ntrain_lc = load_and_merge_data(train_log, is_train=True)\n\nprint(\"üìñ Loading Test Lightcurves...\")\ntest_lc = load_and_merge_data(test_log, is_train=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T14:46:09.287235Z","iopub.execute_input":"2025-12-16T14:46:09.287624Z","iopub.status.idle":"2025-12-16T14:46:10.652607Z","shell.execute_reply.started":"2025-12-16T14:46:09.287590Z","shell.execute_reply":"2025-12-16T14:46:10.651617Z"}},"outputs":[{"name":"stdout","text":"üìñ Loading Metadata (Log files)...\nüìñ Loading Train Lightcurves...\nüìÇ Loading lightcurves from 20 splits...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae5d1ee816fc4321a0a61107625de4b5"}},"metadata":{}},{"name":"stdout","text":"üìñ Loading Test Lightcurves...\nüìÇ Loading lightcurves from 20 splits...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30445ca67331400e898acd7030803178"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# ==========================================\n# 3. PREPROCESSING: EXTINCTION CORRECTION\n# ==========================================\ndef apply_extinction_correction(lc_df, meta_df):\n    \"\"\"\n    Corrects Flux using EBV from metadata.\n    Flux_corr = Flux * 10^(0.4 * R_lambda * EBV)\n    \"\"\"\n    print(\"‚ú® Applying Extinction Correction...\")\n    \n    # Merge EBV into lightcurves\n    lc_df = lc_df.merge(meta_df[['object_id', 'EBV']], on='object_id', how='left')\n    \n    # Map coefficients\n    lc_df['R_lambda'] = lc_df['Filter'].map(Config.EXTINCTION_COEFFS)\n    \n    # Calculate correction factor\n    # A_lambda = R_lambda * EBV\n    # Correction = 10^(0.4 * A_lambda)\n    lc_df['A_lambda'] = lc_df['R_lambda'] * lc_df['EBV']\n    correction_factor = np.power(10, 0.4 * lc_df['A_lambda'])\n    \n    # Apply correction\n    lc_df['Flux'] = lc_df['Flux'] * correction_factor\n    \n    # Clean up\n    lc_df.drop(columns=['EBV', 'R_lambda', 'A_lambda'], inplace=True)\n    return lc_df\n\ntrain_lc = apply_extinction_correction(train_lc, train_log)\ntest_lc = apply_extinction_correction(test_lc, test_log)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T14:46:10.653630Z","iopub.execute_input":"2025-12-16T14:46:10.653884Z","iopub.status.idle":"2025-12-16T14:46:11.159495Z","shell.execute_reply.started":"2025-12-16T14:46:10.653864Z","shell.execute_reply":"2025-12-16T14:46:11.158423Z"}},"outputs":[{"name":"stdout","text":"‚ú® Applying Extinction Correction...\n‚ú® Applying Extinction Correction...\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ==========================================\n# 5. ENHANCED FEATURE ENGINEERING WITH 2D GP\n# All-Band Features\n# ==========================================\n\ndef calculate_von_neumann(flux_series):\n    \"\"\"\n    von Neumann Ratio: Mean Squared Successive Difference / Variance\n    \"\"\"\n    if len(flux_series) < 2: return 0.0\n    return np.mean(np.diff(flux_series)**2) / (np.var(flux_series) + 1e-9)\n\ndef duration(x):\n    return x.max() - x.min()\n\ndef extract_gp_features_all_bands(obj_data, oid):\n    \"\"\"\n    Extract comprehensive 2D Gaussian Process features using ALL bands together.\n    This exploits the full 2D (Time, Wavelength) modeling capability.\n    \"\"\"\n    \n    # Map filters to wavelengths\n    obj_data = obj_data.copy()\n    obj_data['Wavelength'] = obj_data['Filter'].map(GPConfig.WAVELENGTHS)\n    \n    # Clean data - remove NaN/Inf\n    mask = np.isfinite(obj_data['Flux']) & np.isfinite(obj_data['Flux_err'])\n    obj_data = obj_data[mask]\n    \n    # Initialize all features with NaN\n    gp_feats = {'object_id': oid}\n    \n    # Skip if too few points\n    if len(obj_data) < 5:\n        return _create_nan_features(oid)\n    \n    try:\n        # ===== FIT 2D GP ON ALL BANDS SIMULTANEOUSLY =====\n        X = obj_data[['Time (MJD)', 'Wavelength']].values\n        y = obj_data['Flux'].values\n        y_err = obj_data['Flux_err'].values\n        y_mean = np.mean(y)\n        y_std = np.std(y)\n        \n        # Define kernel\n        kernel = Matern(length_scale=GPConfig.LENGTH_SCALE, nu=1.5) + \\\n                 WhiteKernel(noise_level=GPConfig.NOISE_LEVEL)\n        \n        # Fit GP on ALL data points (all bands together)\n        gp = GaussianProcessRegressor(\n            kernel=kernel,\n            alpha=(y_err**2 + 1e-5),\n            normalize_y=False,\n            random_state=Config.SEED\n        )\n        gp.fit(X, y - y_mean)\n        \n        # ========================================\n        # FEATURE GROUP 1: KERNEL HYPERPARAMETERS\n        # ========================================\n        learned_params = gp.kernel_.theta\n        gp_feats['gp_log_likelihood'] = gp.log_marginal_likelihood_value_\n        gp_feats['gp_time_scale'] = np.exp(learned_params[0])\n        gp_feats['gp_wavelength_scale'] = np.exp(learned_params[1])\n        gp_feats['gp_noise_level'] = np.exp(learned_params[2])\n        \n        # Normalized scales (relative to data range)\n        time_range = X[:, 0].max() - X[:, 0].min()\n        wavelength_range = X[:, 1].max() - X[:, 1].min()\n        gp_feats['gp_time_scale_normalized'] = gp_feats['gp_time_scale'] / (time_range + 1e-9)\n        gp_feats['gp_wavelength_scale_normalized'] = gp_feats['gp_wavelength_scale'] / (wavelength_range + 1e-9)\n        \n        # ========================================\n        # FEATURE GROUP 2: RESIDUAL STATISTICS\n        # ========================================\n        y_pred_train = gp.predict(X) + y_mean\n        residuals = y - y_pred_train\n        normalized_residuals = residuals / (y_std + 1e-9)\n        \n        gp_feats['gp_residual_mean'] = np.mean(residuals)\n        gp_feats['gp_residual_std'] = np.std(residuals)\n        gp_feats['gp_residual_median'] = np.median(residuals)\n        gp_feats['gp_residual_skew'] = pd.Series(residuals).skew()\n        gp_feats['gp_residual_kurtosis'] = pd.Series(residuals).kurtosis()\n        gp_feats['gp_max_deviation'] = np.max(np.abs(residuals))\n        gp_feats['gp_mae'] = np.mean(np.abs(residuals))\n        gp_feats['gp_mse'] = np.mean(residuals**2)\n        gp_feats['gp_normalized_residual_std'] = np.std(normalized_residuals)\n        \n        # ========================================\n        # FEATURE GROUP 3: DENSE GRID PREDICTIONS\n        # ========================================\n        time_min, time_max = X[:, 0].min(), X[:, 0].max()\n        time_grid = np.linspace(time_min, time_max, 30)\n        \n        band_predictions = {}\n        band_uncertainties = {}\n        band_derivatives = {}\n        \n        for band, wavelength in GPConfig.WAVELENGTHS.items():\n            X_pred = np.column_stack([time_grid, np.full(len(time_grid), wavelength)])\n            y_pred, y_std = gp.predict(X_pred, return_std=True)\n            y_pred = y_pred + y_mean\n            \n            band_predictions[band] = y_pred\n            band_uncertainties[band] = y_std\n            \n            # Time derivative (rate of change)\n            band_derivatives[band] = np.gradient(y_pred, time_grid)\n        \n        # ========================================\n        # FEATURE GROUP 4: PER-BAND STATISTICS\n        # ========================================\n        for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n            if band not in band_predictions:\n                continue\n            \n            pred = band_predictions[band]\n            uncert = band_uncertainties[band]\n            deriv = band_derivatives[band]\n            \n            # Smoothed statistics\n            gp_feats[f'gp_smoothed_mean_{band}'] = np.mean(pred)\n            gp_feats[f'gp_smoothed_max_{band}'] = np.max(pred)\n            gp_feats[f'gp_smoothed_min_{band}'] = np.min(pred)\n            gp_feats[f'gp_smoothed_range_{band}'] = np.max(pred) - np.min(pred)\n            gp_feats[f'gp_smoothed_std_{band}'] = np.std(pred)\n            \n            # Uncertainty metrics\n            gp_feats[f'gp_uncertainty_mean_{band}'] = np.mean(uncert)\n            gp_feats[f'gp_uncertainty_max_{band}'] = np.max(uncert)\n            gp_feats[f'gp_uncertainty_std_{band}'] = np.std(uncert)\n            \n            # Derivative features (variability characteristics)\n            gp_feats[f'gp_deriv_mean_{band}'] = np.mean(deriv)\n            gp_feats[f'gp_deriv_std_{band}'] = np.std(deriv)\n            gp_feats[f'gp_deriv_max_{band}'] = np.max(deriv)\n            gp_feats[f'gp_deriv_min_{band}'] = np.min(deriv)\n            \n            # Peak timing\n            peak_idx = np.argmax(pred)\n            gp_feats[f'gp_peak_time_{band}'] = time_grid[peak_idx]\n            gp_feats[f'gp_peak_value_{band}'] = pred[peak_idx]\n        \n        # ========================================\n        # FEATURE GROUP 5: COLOR FEATURES\n        # ========================================\n        # Colors at multiple time points (early, middle, late)\n        time_points = [0, len(time_grid)//2, len(time_grid)-1]\n        time_labels = ['early', 'mid', 'late']\n        \n        for time_idx, time_label in zip(time_points, time_labels):\n            if 'u' in band_predictions and 'g' in band_predictions:\n                gp_feats[f'gp_color_ug_{time_label}'] = band_predictions['u'][time_idx] - band_predictions['g'][time_idx]\n            if 'g' in band_predictions and 'r' in band_predictions:\n                gp_feats[f'gp_color_gr_{time_label}'] = band_predictions['g'][time_idx] - band_predictions['r'][time_idx]\n            if 'r' in band_predictions and 'i' in band_predictions:\n                gp_feats[f'gp_color_ri_{time_label}'] = band_predictions['r'][time_idx] - band_predictions['i'][time_idx]\n            if 'i' in band_predictions and 'z' in band_predictions:\n                gp_feats[f'gp_color_iz_{time_label}'] = band_predictions['i'][time_idx] - band_predictions['z'][time_idx]\n        \n        # Color evolution (how colors change over time)\n        if 'u' in band_predictions and 'g' in band_predictions:\n            gp_feats['gp_color_ug_evolution'] = (band_predictions['u'][-1] - band_predictions['g'][-1]) - \\\n                                                 (band_predictions['u'][0] - band_predictions['g'][0])\n        if 'g' in band_predictions and 'r' in band_predictions:\n            gp_feats['gp_color_gr_evolution'] = (band_predictions['g'][-1] - band_predictions['r'][-1]) - \\\n                                                 (band_predictions['g'][0] - band_predictions['r'][0])\n        \n        # ========================================\n        # FEATURE GROUP 6: CROSS-BAND CORRELATIONS\n        # ========================================\n        # Correlation between band lightcurves\n        available_bands = list(band_predictions.keys())\n        for i, band1 in enumerate(available_bands):\n            for band2 in available_bands[i+1:]:\n                corr = np.corrcoef(band_predictions[band1], band_predictions[band2])[0, 1]\n                gp_feats[f'gp_corr_{band1}{band2}'] = corr\n        \n        # ========================================\n        # FEATURE GROUP 7: TEMPORAL FEATURES\n        # ========================================\n        # Using all bands combined\n        all_predictions = np.concatenate([band_predictions[b] for b in band_predictions.keys()])\n        \n        gp_feats['gp_overall_mean'] = np.mean(all_predictions)\n        gp_feats['gp_overall_std'] = np.std(all_predictions)\n        gp_feats['gp_overall_range'] = np.max(all_predictions) - np.min(all_predictions)\n        \n        # Average uncertainty across all bands\n        all_uncertainties = np.concatenate([band_uncertainties[b] for b in band_uncertainties.keys()])\n        gp_feats['gp_overall_uncertainty_mean'] = np.mean(all_uncertainties)\n        gp_feats['gp_overall_uncertainty_std'] = np.std(all_uncertainties)\n        \n        # ========================================\n        # FEATURE GROUP 8: RISE/DECLINE CHARACTERISTICS\n        # ========================================\n        # Analyze each band for rise/decline patterns\n        rise_durations = {}\n        decline_durations = {}\n        peak_times = {}\n        \n        for band in band_predictions.keys():\n            pred = band_predictions[band]\n            peak_idx = np.argmax(pred)\n            peak_times[band] = time_grid[peak_idx]\n            \n            # Rise metrics (before peak)\n            if peak_idx > 0:\n                rise_values = pred[:peak_idx+1]\n                rise_times = time_grid[:peak_idx+1]\n                gp_feats[f'gp_rise_duration_{band}'] = rise_times[-1] - rise_times[0]\n                gp_feats[f'gp_rise_rate_{band}'] = (rise_values[-1] - rise_values[0]) / (gp_feats[f'gp_rise_duration_{band}'] + 1e-9)\n                rise_durations[band] = gp_feats[f'gp_rise_duration_{band}']\n            \n            # Decline metrics (after peak)\n            if peak_idx < len(pred) - 1:\n                decline_values = pred[peak_idx:]\n                decline_times = time_grid[peak_idx:]\n                gp_feats[f'gp_decline_duration_{band}'] = decline_times[-1] - decline_times[0]\n                gp_feats[f'gp_decline_rate_{band}'] = (decline_values[-1] - decline_values[0]) / (gp_feats[f'gp_decline_duration_{band}'] + 1e-9)\n                decline_durations[band] = gp_feats[f'gp_decline_duration_{band}']\n        \n        # CRITICAL TDE FEATURE: Rise/Decline Asymmetry\n        # TDEs have very asymmetric lightcurves (decline >> rise)\n        for band in band_predictions.keys():\n            if band in rise_durations and band in decline_durations:\n                gp_feats[f'gp_asymmetry_ratio_{band}'] = decline_durations[band] / (rise_durations[band] + 1e-9)\n        \n        # Overall asymmetry (average across bands)\n        asymmetry_ratios = [gp_feats[f'gp_asymmetry_ratio_{b}'] for b in band_predictions.keys() \n                           if f'gp_asymmetry_ratio_{b}' in gp_feats]\n        if asymmetry_ratios:\n            gp_feats['gp_mean_asymmetry_ratio'] = np.mean(asymmetry_ratios)\n        \n        # CRITICAL TDE FEATURE: Peak Timing Across Bands\n        # TDEs: bluer bands peak earlier than redder bands\n        if len(peak_times) >= 2:\n            if 'u' in peak_times and 'r' in peak_times:\n                gp_feats['gp_peak_timing_diff_ur'] = peak_times['u'] - peak_times['r']\n            if 'g' in peak_times and 'i' in peak_times:\n                gp_feats['gp_peak_timing_diff_gi'] = peak_times['g'] - peak_times['i']\n            if 'r' in peak_times and 'z' in peak_times:\n                gp_feats['gp_peak_timing_diff_rz'] = peak_times['r'] - peak_times['z']\n            \n            # Measure spread of peak times (AGNs have synchronized \"peaks\", TDEs staggered)\n            gp_feats['gp_peak_timing_spread'] = np.std(list(peak_times.values()))\n        \n        # ========================================\n        # FEATURE GROUP 9: SPECTRAL FEATURES\n        # ========================================\n        # How does flux vary with wavelength at fixed times?\n        wavelengths = np.array([GPConfig.WAVELENGTHS[b] for b in band_predictions.keys()])\n        \n        # Spectral slope at peak time\n        peak_fluxes = np.array([band_predictions[b][len(time_grid)//2] for b in band_predictions.keys()])\n        if len(wavelengths) >= 2:\n            spectral_slope = np.polyfit(wavelengths, peak_fluxes, 1)[0]\n            gp_feats['gp_spectral_slope'] = spectral_slope\n            gp_feats['gp_spectral_curvature'] = np.std(peak_fluxes - np.polyval([spectral_slope, 0], wavelengths))\n        \n        # ========================================\n        # FEATURE GROUP 10: MODEL QUALITY METRICS\n        # ========================================\n        # How well does the GP fit each band?\n        for band in band_predictions.keys():\n            band_mask = obj_data['Filter'] == band\n            if band_mask.sum() > 0:\n                band_data = obj_data[band_mask]\n                X_band = band_data[['Time (MJD)', 'Wavelength']].values\n                y_band = band_data['Flux'].values\n                y_pred_band = gp.predict(X_band) + y_mean\n                \n                residuals_band = y_band - y_pred_band\n                gp_feats[f'gp_band_mse_{band}'] = np.mean(residuals_band**2)\n                gp_feats[f'gp_band_mae_{band}'] = np.mean(np.abs(residuals_band))\n        \n        # ========================================\n        # FEATURE GROUP 11: VARIABILITY METRICS (AGN vs TDE/SN)\n        # ========================================\n        # AGNs show stochastic variability, TDEs/SNe are smooth\n        \n        # Normalized excess variance (high for AGN, low for TDE/SN)\n        all_predictions_flat = np.concatenate([band_predictions[b] for b in band_predictions.keys()])\n        all_uncertainties_flat = np.concatenate([band_uncertainties[b] for b in band_uncertainties.keys()])\n        \n        mean_flux = np.mean(all_predictions_flat)\n        var_flux = np.var(all_predictions_flat)\n        mean_err_sq = np.mean(all_uncertainties_flat**2)\n        \n        excess_variance = (var_flux - mean_err_sq) / (mean_flux**2 + 1e-9)\n        gp_feats['gp_excess_variance'] = max(0, excess_variance)  # Clip negative values\n        \n        # Structure function at multiple timescales (AGN-specific)\n        for band in band_predictions.keys():\n            pred = band_predictions[band]\n            # Calculate structure function at lag = 1, 5, 10 points\n            for lag in [1, 5, 10]:\n                if len(pred) > lag:\n                    sf = np.mean((pred[lag:] - pred[:-lag])**2)\n                    gp_feats[f'gp_structure_function_lag{lag}_{band}'] = sf\n        \n        # ========================================\n        # FEATURE GROUP 12: SECOND DERIVATIVE (CURVATURE)\n        # ========================================\n        # TDEs have smooth curvature, SNe exponential, AGNs irregular\n        for band in band_predictions.keys():\n            pred = band_predictions[band]\n            second_deriv = np.gradient(np.gradient(pred, time_grid), time_grid)\n            \n            gp_feats[f'gp_curvature_mean_{band}'] = np.mean(second_deriv)\n            gp_feats[f'gp_curvature_std_{band}'] = np.std(second_deriv)\n            gp_feats[f'gp_curvature_max_{band}'] = np.max(np.abs(second_deriv))\n        \n        # ========================================\n        # FEATURE GROUP 13: COLOR-MAGNITUDE CORRELATION\n        # ========================================\n        # How do colors correlate with brightness?\n        if 'g' in band_predictions and 'r' in band_predictions:\n            color_gr = band_predictions['g'] - band_predictions['r']\n            magnitude_r = band_predictions['r']\n            \n            corr_color_mag = np.corrcoef(color_gr, magnitude_r)[0, 1]\n            gp_feats['gp_color_magnitude_corr_gr'] = corr_color_mag\n        \n        if 'u' in band_predictions and 'g' in band_predictions:\n            color_ug = band_predictions['u'] - band_predictions['g']\n            magnitude_g = band_predictions['g']\n            \n            corr_color_mag = np.corrcoef(color_ug, magnitude_g)[0, 1]\n            gp_feats['gp_color_magnitude_corr_ug'] = corr_color_mag\n        \n    except Exception as e:\n        # If GP fitting fails, return NaN features\n        return _create_nan_features(oid)\n    \n    return gp_feats\n\ndef _create_nan_features(oid):\n    \"\"\"Create dictionary with all features set to NaN for failed fits\"\"\"\n    features = {'object_id': oid}\n    \n    # Add all possible feature names with NaN values\n    feature_names = [\n        'gp_log_likelihood', 'gp_time_scale', 'gp_wavelength_scale', 'gp_noise_level',\n        'gp_time_scale_normalized', 'gp_wavelength_scale_normalized',\n        'gp_residual_mean', 'gp_residual_std', 'gp_residual_median', 'gp_residual_skew',\n        'gp_residual_kurtosis', 'gp_max_deviation', 'gp_mae', 'gp_mse', \n        'gp_normalized_residual_std', 'gp_overall_mean', 'gp_overall_std',\n        'gp_overall_range', 'gp_overall_uncertainty_mean', 'gp_overall_uncertainty_std',\n        'gp_spectral_slope', 'gp_spectral_curvature'\n    ]\n    \n    for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n        feature_names.extend([\n            f'gp_smoothed_mean_{band}', f'gp_smoothed_max_{band}', f'gp_smoothed_min_{band}',\n            f'gp_smoothed_range_{band}', f'gp_smoothed_std_{band}',\n            f'gp_uncertainty_mean_{band}', f'gp_uncertainty_max_{band}', f'gp_uncertainty_std_{band}',\n            f'gp_deriv_mean_{band}', f'gp_deriv_std_{band}', f'gp_deriv_max_{band}', f'gp_deriv_min_{band}',\n            f'gp_peak_time_{band}', f'gp_peak_value_{band}',\n            f'gp_rise_duration_{band}', f'gp_rise_rate_{band}',\n            f'gp_decline_duration_{band}', f'gp_decline_rate_{band}',\n            f'gp_band_mse_{band}', f'gp_band_mae_{band}'\n        ])\n    \n    for time_label in ['early', 'mid', 'late']:\n        feature_names.extend([\n            f'gp_color_ug_{time_label}', f'gp_color_gr_{time_label}',\n            f'gp_color_ri_{time_label}', f'gp_color_iz_{time_label}'\n        ])\n    \n    feature_names.extend(['gp_color_ug_evolution', 'gp_color_gr_evolution'])\n    \n    # Cross-band correlations\n    bands = ['u', 'g', 'r', 'i', 'z', 'y']\n    for i, b1 in enumerate(bands):\n        for b2 in bands[i+1:]:\n            feature_names.append(f'gp_corr_{b1}{b2}')\n    \n    for name in feature_names:\n        features[name] = np.nan\n    \n    return features\n\ndef extract_features_with_gp(lc_df, meta_df, use_gp=True):\n    \"\"\"\n    Enhanced feature extraction with comprehensive 2D GP features across all bands.\n    \"\"\"\n    print(\"‚öôÔ∏è Extracting Features...\")\n    \n    # ===== PART 1: ORIGINAL FEATURES =====\n    aggs = {\n        'Flux': ['mean', 'max', 'min', 'std', 'skew'],\n        'Flux_err': ['mean'],\n        'Time (MJD)': [duration, 'count']\n    }\n    \n    feats = lc_df.groupby(['object_id', 'Filter']).agg(aggs)\n    feats.columns = ['_'.join(col).strip() for col in feats.columns.values]\n    feats = feats.unstack('Filter')\n    feats.columns = [f\"{c[0]}_{c[1]}\" for c in feats.columns]\n    feats.reset_index(inplace=True)\n    \n    # Von Neumann Ratio\n    vn_ratios = []\n    filters = lc_df['Filter'].unique()\n    \n    for f in filters:\n        f_data = lc_df[lc_df['Filter'] == f]\n        if f_data.empty:\n            continue\n        vn = f_data.groupby('object_id')['Flux'].apply(calculate_von_neumann)\n        vn.name = f'Flux_VonNeumann_{f}'\n        vn_ratios.append(vn)\n    \n    if vn_ratios:\n        vn_df = pd.concat(vn_ratios, axis=1).reset_index()\n        feats = pd.merge(feats, vn_df, on='object_id', how='left')\n    \n    # Merge Metadata\n    meta_df['object_id'] = meta_df['object_id'].astype(str)\n    feats['object_id'] = feats['object_id'].astype(str)\n    \n    merge_cols = ['object_id', 'Z']\n    if 'target' in meta_df.columns:\n        merge_cols.append('target')\n    feats = pd.merge(feats, meta_df[merge_cols], on='object_id', how='left')\n    \n    # Redshift Scaling\n    if 'Z' in feats.columns:\n        for f in filters:\n            col_mean = f'Flux_mean_{f}'\n            col_max = f'Flux_max_{f}'\n            if col_mean in feats.columns:\n                feats[f'Flux_Z_Scaled_{f}'] = feats[col_mean] * (feats['Z']**2)\n            if col_max in feats.columns:\n                feats[f'Flux_Max_Z_Scaled_{f}'] = feats[col_max] * (feats['Z']**2)\n    \n    # ===== PART 2: COMPREHENSIVE 2D GP FEATURES =====\n    if use_gp and Config.PROCESS_GP:\n        print(\"üî¨ Extracting Comprehensive 2D GP Features (All Bands)...\")\n        \n        unique_ids = lc_df['object_id'].unique()\n        gp_features_list = []\n        \n        for oid in tqdm(unique_ids, desc=\"Fitting 2D GPs\"):\n            obj_data = lc_df[lc_df['object_id'] == oid]\n            gp_feats = extract_gp_features_all_bands(obj_data, oid)\n            gp_features_list.append(gp_feats)\n        \n        gp_feats_df = pd.DataFrame(gp_features_list)\n        \n        # Merge GP features\n        feats = pd.merge(feats, gp_feats_df, on='object_id', how='left')\n        \n        n_gp_features = len(gp_feats_df.columns) - 1\n        print(f\"‚úÖ Added {n_gp_features} comprehensive GP-derived features\")\n    \n    return feats\n\n# ==========================================\n# EXECUTION\n# ==========================================\n\nprint(\"Extracting Train Features...\")\ntrain_feats = extract_features_with_gp(train_lc, train_log, use_gp=True)\n\nprint(\"Extracting Test Features...\")\ntest_feats = extract_features_with_gp(test_lc, test_log, use_gp=True)\n\nprint(f\"\\nüìä Feature Summary:\")\nprint(f\"Train shape: {train_feats.shape}\")\nprint(f\"Test shape: {test_feats.shape}\")\nprint(f\"Total features: {len(train_feats.columns)}\")\n\n# Show GP feature categories\ngp_cols = [col for col in train_feats.columns if col.startswith('gp_')]\nprint(f\"\\nGP features: {len(gp_cols)}\")\nprint(f\"Sample GP features: {gp_cols[:10]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T14:46:11.161533Z","iopub.execute_input":"2025-12-16T14:46:11.161841Z","iopub.status.idle":"2025-12-16T15:40:50.413456Z","shell.execute_reply.started":"2025-12-16T14:46:11.161818Z","shell.execute_reply":"2025-12-16T15:40:50.412325Z"}},"outputs":[{"name":"stdout","text":"Extracting Train Features...\n‚öôÔ∏è Extracting Features...\nüî¨ Extracting Comprehensive 2D GP Features (All Bands)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fitting 2D GPs:   0%|          | 0/3043 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8e1de0d9a6e48f4a1713be859fcbd0a"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Added 221 comprehensive GP-derived features\nExtracting Test Features...\n‚öôÔ∏è Extracting Features...\nüî¨ Extracting Comprehensive 2D GP Features (All Bands)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fitting 2D GPs:   0%|          | 0/7135 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9160627515af42deb95365bfa5961f59"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Added 221 comprehensive GP-derived features\n\nüìä Feature Summary:\nTrain shape: (3043, 290)\nTest shape: (7135, 289)\nTotal features: 290\n\nGP features: 221\nSample GP features: ['gp_log_likelihood', 'gp_time_scale', 'gp_wavelength_scale', 'gp_noise_level', 'gp_time_scale_normalized', 'gp_wavelength_scale_normalized', 'gp_residual_mean', 'gp_residual_std', 'gp_residual_median', 'gp_residual_skew']\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ==========================================\n# 6. SAVE\n# ==========================================\nprint(f\"üíæ Saving to Parquet...\")\ntrain_feats.to_parquet(Config.OUTPUT_FILE_TRAIN)\ntest_feats.to_parquet(Config.OUTPUT_FILE_TEST)\n\nprint(\"‚úÖ Processing Complete.\")\nprint(f\"Train Shape: {train_feats.shape}\")\nprint(f\"Test Shape: {test_feats.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T15:40:50.414777Z","iopub.execute_input":"2025-12-16T15:40:50.415282Z","iopub.status.idle":"2025-12-16T15:40:50.868979Z","shell.execute_reply.started":"2025-12-16T15:40:50.415246Z","shell.execute_reply":"2025-12-16T15:40:50.867955Z"}},"outputs":[{"name":"stdout","text":"üíæ Saving to Parquet...\n‚úÖ Processing Complete.\nTrain Shape: (3043, 290)\nTest Shape: (7135, 289)\n","output_type":"stream"}],"execution_count":17}]}