{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24c7e64",
   "metadata": {},
   "source": [
    "True Flux formula:\n",
    "$$F_{int}(\\lambda) = F_{obs}(\\lambda) \\times 10^{0.4 \\cdot A_\\lambda}$$\n",
    "\n",
    "Where:\n",
    "$$A_\\lambda = E(B-V) \\cdot [R_V \\cdot a(\\lambda) + b(\\lambda)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52082912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "RAW_DATA_PATH = Path(\"../data/raw\")\n",
    "PROCESSED_DATA_PATH = Path(\"../data/processed\")\n",
    "\n",
    "# Extinction coefficients for different filters\n",
    "# Used for calculating true Flux\n",
    "EXTINCTION_COEFFS = {\n",
    "    'u': 4.81,\n",
    "    'g': 3.64,\n",
    "    'r': 2.70,\n",
    "    'i': 2.06,\n",
    "    'z': 1.58,\n",
    "    'y': 1.31\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77ee98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tackle faint TDEs\n",
    "def calculate_temperature_proxy(df):\n",
    "    # 1. Mean Color (Static Temperature)\n",
    "    # Higher value = Bluer (Hotter)\n",
    "    # If 'u' is much brighter than 'g', this ratio is high.\n",
    "    df['color_u_div_g'] = df['Flux_corrected_mean_u'] / (df['Flux_corrected_mean_g'] + 1e-6)\n",
    "\n",
    "    # 2. Color Evolution (The \"Cooling Rate\")\n",
    "    # TDEs cool slowly (stay blue). SNe cool fast (turn red).\n",
    "    # We compare color at Peak vs. Color Late-time.\n",
    "    \n",
    "    # Simple proxy: (Max u / Max g) - (Mean u / Mean g)\n",
    "    # If positive: It was bluer at peak than on average (Typical behavior)\n",
    "    # If near zero: Temperature stayed constant (TDE characteristic)\n",
    "    ratio_peak = df['Flux_corrected_max_u'] / (df['Flux_corrected_max_g'] + 1e-6)\n",
    "    ratio_mean = df['Flux_corrected_mean_u'] / (df['Flux_corrected_mean_g'] + 1e-6)\n",
    "    \n",
    "    df['color_change_u_g'] = ratio_peak - ratio_mean\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ef7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shape_features(df):\n",
    "    # \"Plumpness\" (Area under curve relative to Peak)\n",
    "    # A spike is thin. A sustained event is plump.\n",
    "    # Formula: Mean Flux / Max Flux\n",
    "    # For a square wave = 1.0. For a delta spike ~ 0.\n",
    "    \n",
    "    for band in ['u', 'g', 'r', 'i', 'z', 'y']:\n",
    "        col_mean = f'Flux_corrected_mean_{band}'\n",
    "        col_max = f'Flux_corrected_max_{band}'\n",
    "        \n",
    "        # This feature is 100% independent of distance/brightness\n",
    "        # If it's a TDE, this value should be consistent whether it's z=0.1 or z=1.0\n",
    "        df[f'shape_plumpness_{band}'] = df[col_mean] / (df[col_max] + 1e-6)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb4c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simplified \"Decay Rate\" feature\n",
    "# Compare Flux at Peak vs Flux at the end of the window\n",
    "\n",
    "def calculate_decay_strength(df):\n",
    "    for band in ['u', 'g']: # Focus on blue bands where TDEs live\n",
    "        # Ratio of Min (late-time) to Max (peak)\n",
    "        # Low value = Faded away completely\n",
    "        # High value = Still glowing (Plateau)\n",
    "        col_min = f'Flux_corrected_min_{band}'\n",
    "        col_max = f'Flux_corrected_max_{band}'\n",
    "        \n",
    "        df[f'decay_ratio_{band}'] = df[col_min] / (df[col_max] + 1e-6)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c3a5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(raw_path, train=True):\n",
    "    # Dunno what to call this\n",
    "    if train:\n",
    "        something = \"train_\"\n",
    "    else:\n",
    "        something = \"test_\"\n",
    "    \n",
    "    # Master metadata\n",
    "    log_file_path = raw_path / (something + \"log.csv\")\n",
    "    log_df = pd.read_csv(log_file_path)\n",
    "    \n",
    "    # Drop the \"for fun\" column and split name column\n",
    "    log_df = log_df.drop(columns=[\"English Translation\", \"split\", \"SpecType\"], errors='ignore')\n",
    "\n",
    "    # Stores processed data chunks\n",
    "    processed_chunks = []\n",
    "\n",
    "    # Loop through 20 splits\n",
    "    for i in range (1, 21):\n",
    "        folder_name = f\"split_{i:02d}\" # Formats to 'split_01', 'split_02'\n",
    "        file_path = raw_path / folder_name / (something + \"full_lightcurves.csv\")\n",
    "        \n",
    "        print(f\"Processing {folder_name}...\")\n",
    "        \n",
    "        # Load lightcurves data for this split only\n",
    "        chunk_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Drop rows with missing Flux values\n",
    "        chunk_df = chunk_df.dropna(subset=['Flux'])\n",
    "        \n",
    "        # Merge metadata with lightcurves on 'object_id'\n",
    "        chunk_df = chunk_df.merge(log_df, on=\"object_id\", how=\"left\")\n",
    "        \n",
    "        # == Step 1: Extinction Correction ==\n",
    "        # ======================================================\n",
    "        \n",
    "        # Calculate total extinction for each row\n",
    "        extinction_factors = chunk_df[\"Filter\"].map(EXTINCTION_COEFFS)\n",
    "        \n",
    "        chunk_df[\"Total_extinction\"] = extinction_factors * chunk_df[\"EBV\"]\n",
    "        \n",
    "        # Calculate extinction-corrected flux and flux error\n",
    "        chunk_df[\"Flux_corrected\"] = chunk_df[\"Flux\"] * 10**(0.4 * chunk_df[\"Total_extinction\"])\n",
    "        chunk_df[\"Flux_err_corrected\"] = chunk_df[\"Flux_err\"] * 10**(0.4 * chunk_df[\"Total_extinction\"])\n",
    "        \n",
    "        # == Calculate Per-Observation SNR ==\n",
    "        chunk_df[\"SNR_obs\"] = chunk_df[\"Flux_corrected\"] / chunk_df[\"Flux_err_corrected\"]\n",
    "        \n",
    "        # == Step 2: Extract time features ==\n",
    "        # ======================================================\n",
    "        \n",
    "        # Sort by object_id and then MJD\n",
    "        chunk_df = chunk_df.sort_values(by=[\"object_id\", \"Time (MJD)\"])\n",
    "        \n",
    "        # Set the starting time of observation to zero for each object\n",
    "        start_times = chunk_df.groupby('object_id')['Time (MJD)'].transform('min')\n",
    "        chunk_df['Time_relative'] = chunk_df['Time (MJD)'] - start_times\n",
    "        \n",
    "        # Calculate delta time between observations of the object on the same filter\n",
    "        chunk_df[\"Delta_time\"] = chunk_df.groupby([\"object_id\", \"Filter\"])[\"Time_relative\"].diff().fillna(0)\n",
    "        \n",
    "        # Calculate delta flux between observations of the object on the same filter\n",
    "        chunk_df[\"Delta_flux\"] = chunk_df.groupby([\"object_id\", \"Filter\"])[\"Flux_corrected\"].diff().fillna(0)\n",
    "        \n",
    "        # Calculate flux change rate\n",
    "        chunk_df[\"Flux_change_rate\"] = chunk_df[\"Delta_flux\"] / chunk_df[\"Delta_time\"].replace(0, np.nan)\n",
    "        chunk_df[\"Flux_change_rate\"] = chunk_df[\"Flux_change_rate\"].fillna(0)\n",
    "        \n",
    "        # == Step 3: Extract statistical features ==\n",
    "        # ======================================================\n",
    "        \n",
    "        # -- 1. Extract Peak and Min Times --\n",
    "        # We need indices for both Max and Min to get the corresponding Times\n",
    "        idx_max = chunk_df.groupby(['object_id', 'Filter'])['Flux_corrected'].idxmax().dropna()\n",
    "        idx_min = chunk_df.groupby(['object_id', 'Filter'])['Flux_corrected'].idxmin().dropna()\n",
    "        \n",
    "        # Extract Time of Max\n",
    "        time_max_data = chunk_df.loc[idx_max, ['object_id', 'Filter', 'Time_relative']]\n",
    "        time_max_wide = time_max_data.pivot(index='object_id', columns='Filter', values='Time_relative')\n",
    "        time_max_wide = time_max_wide.rename(columns=lambda x: f\"Time_of_max_flux_{x}\")\n",
    "        \n",
    "        # Extract Time of Min\n",
    "        time_min_data = chunk_df.loc[idx_min, ['object_id', 'Filter', 'Time_relative']]\n",
    "        time_min_wide = time_min_data.pivot(index='object_id', columns='Filter', values='Time_relative')\n",
    "        time_min_wide = time_min_wide.rename(columns=lambda x: f\"Time_of_min_flux_{x}\")\n",
    "\n",
    "        # -- 2. Extract aggregated statistics --\n",
    "        aggs = {\n",
    "            'Flux_corrected': ['mean', 'max', 'min', 'std', 'skew'],\n",
    "            'Flux_change_rate': ['mean', 'max', 'min', 'std'], \n",
    "            'Time_relative': ['max', 'count'], \n",
    "            'Flux_err_corrected': ['mean'],\n",
    "            'SNR_obs': ['mean', 'max']\n",
    "        }\n",
    "        \n",
    "        agg_df = chunk_df.groupby(['object_id', 'Filter']).agg(aggs)\n",
    "        features_df = agg_df.unstack(level='Filter')\n",
    "        \n",
    "        # Flatten Hierarchical Columns\n",
    "        new_columns = []\n",
    "        for col_name, stat_name, filter_name in features_df.columns:\n",
    "            new_name = f\"{col_name}_{stat_name}_{filter_name}\"\n",
    "            new_columns.append(new_name)\n",
    "        \n",
    "        features_df.columns = new_columns\n",
    "        \n",
    "        # Fill NaN counts with 0\n",
    "        count_cols = [col for col in features_df.columns if col.endswith('_count_' + col.split('_')[-1])]\n",
    "        features_df[count_cols] = features_df[count_cols].fillna(0)\n",
    "        \n",
    "        features_df = features_df.reset_index()\n",
    "        \n",
    "        # Merge Time of Max and Time of Min\n",
    "        features_df = features_df.merge(time_max_wide, on='object_id', how='left')\n",
    "        features_df = features_df.merge(time_min_wide, on='object_id', how='left')\n",
    "\n",
    "        # == Step 4: Feature Engineering (Math & Ratios) ==\n",
    "        # ======================================================\n",
    "        \n",
    "        bands = ['u', 'g', 'r', 'i', 'z', 'y']\n",
    "        \n",
    "        # 1. Rise-to-Decay Ratio\n",
    "        # Formula: (Time_Max - Time_Start) / (Time_End - Time_Max)\n",
    "        # Note: Time_Start is always 0 in 'Time_relative', so numerator is just Time_of_max\n",
    "        for b in bands:\n",
    "            t_max_col = f\"Time_of_max_flux_{b}\"\n",
    "            t_total_col = f\"Time_relative_max_{b}\" # Total duration of this filter\n",
    "            \n",
    "            if t_max_col in features_df.columns and t_total_col in features_df.columns:\n",
    "                # Time to Rise = t_max\n",
    "                # Time to Decay = t_total - t_max\n",
    "                # Add epsilon to prevent divide by zero\n",
    "                features_df[f\"rise_decay_ratio_{b}\"] = features_df[t_max_col] / (\n",
    "                    (features_df[t_total_col] - features_df[t_max_col]) + 1e-6\n",
    "                )\n",
    "\n",
    "        # 2. Color Indices\n",
    "        for b1, b2 in zip(bands[:-1], bands[1:]):\n",
    "            col_1 = f\"Flux_corrected_mean_{b1}\"\n",
    "            col_2 = f\"Flux_corrected_mean_{b2}\"\n",
    "            if col_1 in features_df.columns and col_2 in features_df.columns:\n",
    "                features_df[f\"color_{b1}_{b2}\"] = features_df[col_1] - features_df[col_2]\n",
    "\n",
    "        # 3. Amplitude\n",
    "        for b in bands:\n",
    "            max_col = f\"Flux_corrected_max_{b}\"\n",
    "            min_col = f\"Flux_corrected_min_{b}\"\n",
    "            if max_col in features_df.columns and min_col in features_df.columns:\n",
    "                features_df[f\"amplitude_{b}\"] = features_df[max_col] - features_df[min_col]\n",
    "\n",
    "        # 4. Global Duration\n",
    "        time_max_cols = [c for c in features_df.columns if \"Time_relative_max\" in c]\n",
    "        if time_max_cols:\n",
    "             features_df['duration_global'] = features_df[time_max_cols].max(axis=1)\n",
    "\n",
    "        # ======================================================\n",
    "        \n",
    "        # == Step 5: Advanced Feature Calculations ==\n",
    "        # ======================================================\n",
    "        \n",
    "        # Temperature Proxy Features\n",
    "        features_df = calculate_temperature_proxy(features_df)\n",
    "        \n",
    "        # Shape Features\n",
    "        features_df = calculate_shape_features(features_df)\n",
    "        \n",
    "        # Decay Strength Features\n",
    "        features_df = calculate_decay_strength(features_df)\n",
    "        \n",
    "        processed_chunks.append(features_df)\n",
    "        \n",
    "        # Clean up\n",
    "        del chunk_df, agg_df, features_df, time_max_data, time_max_wide, time_min_data, time_min_wide\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"Concatenating chunks...\")\n",
    "    final_df = pd.concat(processed_chunks)\n",
    "    \n",
    "    # Merge with log_df to get target labels\n",
    "    if train:\n",
    "        final_df = final_df.merge(log_df[['object_id', 'Z', 'Z_err', 'target']], on='object_id', how='left')\n",
    "    else:\n",
    "        final_df = final_df.merge(log_df[['object_id', 'Z', 'Z_err']], on='object_id', how='left')\n",
    "    \n",
    "    print(\"Processing complete.\")\n",
    "    \n",
    "    # Clean up unnecessary columns\n",
    "    final_df.drop(columns=['Time (MJD)', 'Flux', 'Flux_err', 'Total_extinction'], errors='ignore', inplace=True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1271e9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split_01...\n",
      "Processing split_02...\n",
      "Processing split_03...\n",
      "Processing split_04...\n",
      "Processing split_05...\n",
      "Processing split_06...\n",
      "Processing split_07...\n",
      "Processing split_08...\n",
      "Processing split_09...\n",
      "Processing split_10...\n",
      "Processing split_11...\n",
      "Processing split_12...\n",
      "Processing split_13...\n",
      "Processing split_14...\n",
      "Processing split_15...\n",
      "Processing split_16...\n",
      "Processing split_17...\n",
      "Processing split_18...\n",
      "Processing split_19...\n",
      "Processing split_20...\n",
      "Concatenating chunks...\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Process training data\n",
    "train_features = feature_extraction(RAW_DATA_PATH, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c763bd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>Flux_corrected_mean_g</th>\n",
       "      <th>Flux_corrected_mean_i</th>\n",
       "      <th>Flux_corrected_mean_r</th>\n",
       "      <th>Flux_corrected_mean_u</th>\n",
       "      <th>Flux_corrected_mean_y</th>\n",
       "      <th>Flux_corrected_mean_z</th>\n",
       "      <th>Flux_corrected_max_g</th>\n",
       "      <th>Flux_corrected_max_i</th>\n",
       "      <th>Flux_corrected_max_r</th>\n",
       "      <th>...</th>\n",
       "      <th>shape_plumpness_g</th>\n",
       "      <th>shape_plumpness_r</th>\n",
       "      <th>shape_plumpness_i</th>\n",
       "      <th>shape_plumpness_z</th>\n",
       "      <th>shape_plumpness_y</th>\n",
       "      <th>decay_ratio_u</th>\n",
       "      <th>decay_ratio_g</th>\n",
       "      <th>Z</th>\n",
       "      <th>Z_err</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dornhoth_fervain_onodrim</td>\n",
       "      <td>-0.541341</td>\n",
       "      <td>2.601577</td>\n",
       "      <td>1.412578</td>\n",
       "      <td>0.950745</td>\n",
       "      <td>-0.458879</td>\n",
       "      <td>1.489760</td>\n",
       "      <td>1.424000</td>\n",
       "      <td>28.277937</td>\n",
       "      <td>13.802700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.380155</td>\n",
       "      <td>0.102341</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.068501</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>-0.868267</td>\n",
       "      <td>3.0490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dornhoth_galadh_ylf</td>\n",
       "      <td>0.218875</td>\n",
       "      <td>0.394725</td>\n",
       "      <td>0.317303</td>\n",
       "      <td>0.034930</td>\n",
       "      <td>0.757253</td>\n",
       "      <td>0.573209</td>\n",
       "      <td>1.518313</td>\n",
       "      <td>5.797249</td>\n",
       "      <td>3.027457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144156</td>\n",
       "      <td>0.104808</td>\n",
       "      <td>0.068088</td>\n",
       "      <td>0.072869</td>\n",
       "      <td>0.062070</td>\n",
       "      <td>-0.753647</td>\n",
       "      <td>-0.224853</td>\n",
       "      <td>0.4324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elrim_melethril_thul</td>\n",
       "      <td>3.757406</td>\n",
       "      <td>7.781453</td>\n",
       "      <td>5.462329</td>\n",
       "      <td>0.078012</td>\n",
       "      <td>0.065533</td>\n",
       "      <td>9.287801</td>\n",
       "      <td>8.691179</td>\n",
       "      <td>14.758927</td>\n",
       "      <td>11.552149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432324</td>\n",
       "      <td>0.472841</td>\n",
       "      <td>0.527237</td>\n",
       "      <td>0.606077</td>\n",
       "      <td>0.005052</td>\n",
       "      <td>-1.164189</td>\n",
       "      <td>0.030279</td>\n",
       "      <td>0.4673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ithil_tobas_rodwen</td>\n",
       "      <td>0.289940</td>\n",
       "      <td>0.455992</td>\n",
       "      <td>0.448044</td>\n",
       "      <td>0.161333</td>\n",
       "      <td>0.348274</td>\n",
       "      <td>0.540509</td>\n",
       "      <td>1.425550</td>\n",
       "      <td>2.269820</td>\n",
       "      <td>1.922080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203388</td>\n",
       "      <td>0.233104</td>\n",
       "      <td>0.200894</td>\n",
       "      <td>0.177858</td>\n",
       "      <td>0.064116</td>\n",
       "      <td>-0.905091</td>\n",
       "      <td>-0.334633</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mirion_adar_Druadan</td>\n",
       "      <td>0.056120</td>\n",
       "      <td>0.428605</td>\n",
       "      <td>0.242114</td>\n",
       "      <td>-0.018189</td>\n",
       "      <td>0.308948</td>\n",
       "      <td>0.322406</td>\n",
       "      <td>1.798828</td>\n",
       "      <td>6.010829</td>\n",
       "      <td>2.709919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031198</td>\n",
       "      <td>0.089344</td>\n",
       "      <td>0.071306</td>\n",
       "      <td>0.109675</td>\n",
       "      <td>0.121521</td>\n",
       "      <td>-1.562661</td>\n",
       "      <td>-0.741583</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  object_id  Flux_corrected_mean_g  Flux_corrected_mean_i  \\\n",
       "0  Dornhoth_fervain_onodrim              -0.541341               2.601577   \n",
       "1       Dornhoth_galadh_ylf               0.218875               0.394725   \n",
       "2      Elrim_melethril_thul               3.757406               7.781453   \n",
       "3        Ithil_tobas_rodwen               0.289940               0.455992   \n",
       "4       Mirion_adar_Druadan               0.056120               0.428605   \n",
       "\n",
       "   Flux_corrected_mean_r  Flux_corrected_mean_u  Flux_corrected_mean_y  \\\n",
       "0               1.412578               0.950745              -0.458879   \n",
       "1               0.317303               0.034930               0.757253   \n",
       "2               5.462329               0.078012               0.065533   \n",
       "3               0.448044               0.161333               0.348274   \n",
       "4               0.242114              -0.018189               0.308948   \n",
       "\n",
       "   Flux_corrected_mean_z  Flux_corrected_max_g  Flux_corrected_max_i  \\\n",
       "0               1.489760              1.424000             28.277937   \n",
       "1               0.573209              1.518313              5.797249   \n",
       "2               9.287801              8.691179             14.758927   \n",
       "3               0.540509              1.425550              2.269820   \n",
       "4               0.322406              1.798828              6.010829   \n",
       "\n",
       "   Flux_corrected_max_r  ...  shape_plumpness_g  shape_plumpness_r  \\\n",
       "0             13.802700  ...          -0.380155           0.102341   \n",
       "1              3.027457  ...           0.144156           0.104808   \n",
       "2             11.552149  ...           0.432324           0.472841   \n",
       "3              1.922080  ...           0.203388           0.233104   \n",
       "4              2.709919  ...           0.031198           0.089344   \n",
       "\n",
       "   shape_plumpness_i  shape_plumpness_z  shape_plumpness_y  decay_ratio_u  \\\n",
       "0           0.092000           0.050680          -0.068501       0.000166   \n",
       "1           0.068088           0.072869           0.062070      -0.753647   \n",
       "2           0.527237           0.606077           0.005052      -1.164189   \n",
       "3           0.200894           0.177858           0.064116      -0.905091   \n",
       "4           0.071306           0.109675           0.121521      -1.562661   \n",
       "\n",
       "   decay_ratio_g       Z  Z_err  target  \n",
       "0      -0.868267  3.0490    NaN       0  \n",
       "1      -0.224853  0.4324    NaN       0  \n",
       "2       0.030279  0.4673    NaN       0  \n",
       "3      -0.334633  0.6946    NaN       0  \n",
       "4      -0.741583  0.4161    NaN       0  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952584e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data is correctly aggregated. Shape: (3043, 128)\n"
     ]
    }
   ],
   "source": [
    "# Verify that we have collapsed the time-series\n",
    "is_unique = train_features.index.is_unique  # If object_id is the index\n",
    "# OR\n",
    "# is_unique = train_features['object_id'].is_unique \n",
    "\n",
    "if is_unique:\n",
    "    print(f\"Data is correctly aggregated. Shape: {train_features.shape}\")\n",
    "else:\n",
    "    print(\"CRITICAL ERROR: You still have multiple rows per object.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19c4235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns are: ['object_id', 'Flux_corrected_mean_g', 'Flux_corrected_mean_i', 'Flux_corrected_mean_r', 'Flux_corrected_mean_u', 'Flux_corrected_mean_y', 'Flux_corrected_mean_z', 'Flux_corrected_max_g', 'Flux_corrected_max_i', 'Flux_corrected_max_r', 'Flux_corrected_max_u', 'Flux_corrected_max_y', 'Flux_corrected_max_z', 'Flux_corrected_min_g', 'Flux_corrected_min_i', 'Flux_corrected_min_r', 'Flux_corrected_min_u', 'Flux_corrected_min_y', 'Flux_corrected_min_z', 'Flux_corrected_std_g', 'Flux_corrected_std_i', 'Flux_corrected_std_r', 'Flux_corrected_std_u', 'Flux_corrected_std_y', 'Flux_corrected_std_z', 'Flux_corrected_skew_g', 'Flux_corrected_skew_i', 'Flux_corrected_skew_r', 'Flux_corrected_skew_u', 'Flux_corrected_skew_y', 'Flux_corrected_skew_z', 'Flux_change_rate_mean_g', 'Flux_change_rate_mean_i', 'Flux_change_rate_mean_r', 'Flux_change_rate_mean_u', 'Flux_change_rate_mean_y', 'Flux_change_rate_mean_z', 'Flux_change_rate_max_g', 'Flux_change_rate_max_i', 'Flux_change_rate_max_r', 'Flux_change_rate_max_u', 'Flux_change_rate_max_y', 'Flux_change_rate_max_z', 'Flux_change_rate_min_g', 'Flux_change_rate_min_i', 'Flux_change_rate_min_r', 'Flux_change_rate_min_u', 'Flux_change_rate_min_y', 'Flux_change_rate_min_z', 'Flux_change_rate_std_g', 'Flux_change_rate_std_i', 'Flux_change_rate_std_r', 'Flux_change_rate_std_u', 'Flux_change_rate_std_y', 'Flux_change_rate_std_z', 'Time_relative_max_g', 'Time_relative_max_i', 'Time_relative_max_r', 'Time_relative_max_u', 'Time_relative_max_y', 'Time_relative_max_z', 'Time_relative_count_g', 'Time_relative_count_i', 'Time_relative_count_r', 'Time_relative_count_u', 'Time_relative_count_y', 'Time_relative_count_z', 'Flux_err_corrected_mean_g', 'Flux_err_corrected_mean_i', 'Flux_err_corrected_mean_r', 'Flux_err_corrected_mean_u', 'Flux_err_corrected_mean_y', 'Flux_err_corrected_mean_z', 'SNR_obs_mean_g', 'SNR_obs_mean_i', 'SNR_obs_mean_r', 'SNR_obs_mean_u', 'SNR_obs_mean_y', 'SNR_obs_mean_z', 'SNR_obs_max_g', 'SNR_obs_max_i', 'SNR_obs_max_r', 'SNR_obs_max_u', 'SNR_obs_max_y', 'SNR_obs_max_z', 'Time_of_max_flux_g', 'Time_of_max_flux_i', 'Time_of_max_flux_r', 'Time_of_max_flux_u', 'Time_of_max_flux_y', 'Time_of_max_flux_z', 'Time_of_min_flux_g', 'Time_of_min_flux_i', 'Time_of_min_flux_r', 'Time_of_min_flux_u', 'Time_of_min_flux_y', 'Time_of_min_flux_z', 'rise_decay_ratio_u', 'rise_decay_ratio_g', 'rise_decay_ratio_r', 'rise_decay_ratio_i', 'rise_decay_ratio_z', 'rise_decay_ratio_y', 'color_u_g', 'color_g_r', 'color_r_i', 'color_i_z', 'color_z_y', 'amplitude_u', 'amplitude_g', 'amplitude_r', 'amplitude_i', 'amplitude_z', 'amplitude_y', 'duration_global', 'color_u_div_g', 'color_change_u_g', 'shape_plumpness_u', 'shape_plumpness_g', 'shape_plumpness_r', 'shape_plumpness_i', 'shape_plumpness_z', 'shape_plumpness_y', 'decay_ratio_u', 'decay_ratio_g', 'Z', 'Z_err', 'target']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available columns are:\", train_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3f508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split_01...\n",
      "Processing split_02...\n",
      "Processing split_03...\n",
      "Processing split_04...\n",
      "Processing split_05...\n",
      "Processing split_06...\n",
      "Processing split_07...\n",
      "Processing split_08...\n",
      "Processing split_09...\n",
      "Processing split_10...\n",
      "Processing split_11...\n",
      "Processing split_12...\n",
      "Processing split_13...\n",
      "Processing split_14...\n",
      "Processing split_15...\n",
      "Processing split_16...\n",
      "Processing split_17...\n",
      "Processing split_18...\n",
      "Processing split_19...\n",
      "Processing split_20...\n",
      "Concatenating chunks...\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "test_features = feature_extraction(RAW_DATA_PATH, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd60109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.to_parquet(PROCESSED_DATA_PATH / \"train_features.parquet\", index=False)\n",
    "test_features.to_parquet(PROCESSED_DATA_PATH / \"test_features.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLFinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
