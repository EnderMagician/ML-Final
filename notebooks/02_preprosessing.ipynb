{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24c7e64",
   "metadata": {},
   "source": [
    "True Flux formula:\n",
    "$$F_{int}(\\lambda) = F_{obs}(\\lambda) \\times 10^{0.4 \\cdot A_\\lambda}$$\n",
    "\n",
    "Where:\n",
    "$$A_\\lambda = E(B-V) \\cdot [R_V \\cdot a(\\lambda) + b(\\lambda)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52082912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "RAW_DATA_PATH = Path(\"../data/raw\")\n",
    "PROCESSED_DATA_PATH = Path(\"../data/processed\")\n",
    "\n",
    "# Extinction coefficients for different filters\n",
    "# Used for calculating true Flux\n",
    "EXTINCTION_COEFFS = {\n",
    "    'u': 4.81,\n",
    "    'g': 3.64,\n",
    "    'r': 2.70,\n",
    "    'i': 2.06,\n",
    "    'z': 1.58,\n",
    "    'y': 1.31\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process data\n",
    "def feature_extraction(raw_path, train=True):\n",
    "    # Dunno what to call this\n",
    "    if train:\n",
    "        something = \"train_\"\n",
    "    else:\n",
    "        something = \"test_\"\n",
    "    \n",
    "    # Master metadata\n",
    "    log_file_path = raw_path / (something + \"log.csv\")\n",
    "    log_df = pd.read_csv(log_file_path)\n",
    "    \n",
    "    # Drop the \"for fun\" column and split name column\n",
    "    log_df = log_df.drop(columns=[\"English Translation\", \"split\", \"SpecType\"])\n",
    "\n",
    "    # Stores processed data chunks\n",
    "    processed_chunks = []\n",
    "\n",
    "    # Loop through 20 splits\n",
    "    for i in range (1, 21):\n",
    "        folder_name = f\"split_{i:02d}\" # Formats to 'split_01', 'split_02'\n",
    "        file_path = raw_path / folder_name / (something + \"full_lightcurves.csv\")\n",
    "        \n",
    "        print(f\"Processing {folder_name}...\")\n",
    "        \n",
    "        # Load lightcurves data for this split only\n",
    "        chunk_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Drop rows with missing Flux values\n",
    "        chunk_df = chunk_df.dropna(subset=['Flux'])\n",
    "        \n",
    "        # Merge metadata with lightcurves on 'object_id'\n",
    "        chunk_df = chunk_df.merge(log_df, on=\"object_id\", how=\"left\")\n",
    "        \n",
    "        # == Step 1: Extinction Correction ==\n",
    "        # ======================================================\n",
    "        \n",
    "        # Calculate total extinction for each row\n",
    "        extinction_factors = chunk_df[\"Filter\"].map(EXTINCTION_COEFFS)\n",
    "        \n",
    "        chunk_df[\"Total_extinction\"] = extinction_factors * chunk_df[\"EBV\"]\n",
    "        \n",
    "        # Calculate extinction-corrected flux and flux error\n",
    "        chunk_df[\"Flux_corrected\"] = chunk_df[\"Flux\"] * 10**(0.4 * chunk_df[\"Total_extinction\"])\n",
    "        chunk_df[\"Flux_err_corrected\"] = chunk_df[\"Flux_err\"] * 10**(0.4 * chunk_df[\"Total_extinction\"])\n",
    "        \n",
    "        # == Step 2: Extract time features ==\n",
    "        # ======================================================\n",
    "        \n",
    "        # Sort by object_id and then MJD\n",
    "        # Makes it easier to compute time differences\n",
    "        chunk_df = chunk_df.sort_values(by=[\"object_id\", \"Time (MJD)\"])\n",
    "        \n",
    "        # Set the starting time of observation to zero for each object\n",
    "        start_times = chunk_df.groupby('object_id')['Time (MJD)'].transform('min')\n",
    "        chunk_df['Time_relative'] = chunk_df['Time (MJD)'] - start_times\n",
    "        \n",
    "        # Calculate delta time between observations of the object on the same filter\n",
    "        # Fill NaN values (first observation) with 0\n",
    "        chunk_df[\"Delta_time\"] = chunk_df.groupby([\"object_id\", \"Filter\"])[\"Time_relative\"].diff().fillna(0)\n",
    "        \n",
    "        # Calculate delta flux between observations of the object on the same filter\n",
    "        # Fill NaN values (first observation) with 0\n",
    "        chunk_df[\"Delta_flux\"] = chunk_df.groupby([\"object_id\", \"Filter\"])[\"Flux_corrected\"].diff().fillna(0)\n",
    "        \n",
    "        # Calculate flux change rate (The most important feature!)\n",
    "        # Fill NaN values (first observation) with 0\n",
    "        chunk_df[\"Flux_change_rate\"] = chunk_df[\"Delta_flux\"] / chunk_df[\"Delta_time\"].replace(0, np.nan)\n",
    "        chunk_df[\"Flux_change_rate\"] = chunk_df[\"Flux_change_rate\"].fillna(0)\n",
    "        \n",
    "        # == Step 3: Extract statistical features ==\n",
    "        # ======================================================\n",
    "        \n",
    "        # -- Extract Peak Time --\n",
    "        \n",
    "        # This returns a list of row numbers (e.g., Row 5, Row 102, Row 888...)\n",
    "        idx_peaks = chunk_df.groupby(['object_id', 'Filter'])['Flux_corrected'].idxmax()\n",
    "    \n",
    "        # .loc takes the list of addresses and returns only those rows\n",
    "        # Only keep the columns we care about: ID, Filter, and the Time\n",
    "        peak_data = chunk_df.loc[idx_peaks, ['object_id', 'Filter', 'Time_relative']]\n",
    "        \n",
    "        # Rename the Time_relative column to Peak_time\n",
    "        peak_data = peak_data.rename(columns={\"Time_relative\": \"Time_to_peak\"})\n",
    "        \n",
    "        # Pivot the table to have filters as columns\n",
    "        peak_data_wide = peak_data.pivot(index='object_id', columns='Filter', values='Time_to_peak')\n",
    "        \n",
    "        # Rename columns to indicate they are peak times\n",
    "        peak_data_wide = peak_data_wide.rename(columns=lambda x: f\"Time_to_peak_{x}\")\n",
    "        \n",
    "        \"\"\" \n",
    "        Before pivot:\n",
    "            object_id   Filter  Time_to_peak\n",
    "            A           g       12.34\n",
    "            A           r       15.67\n",
    "            \n",
    "        After pivot:\n",
    "            object_id   Time_to_peak_g  Time_to_peak_r\n",
    "            A           12.34           15.67\n",
    "        \"\"\"\n",
    "        \n",
    "        # -- Extract aggregated statistics --\n",
    "        \n",
    "        # Define aggregation functions\n",
    "        aggs = {\n",
    "            # 1. Flux Statistics\n",
    "            'Flux_corrected': ['mean', 'max', 'min', 'std', 'skew'],\n",
    "    \n",
    "            # 2. Derivative Statistics\n",
    "            'Flux_change_rate': ['mean', 'max', 'min', 'std'], \n",
    "    \n",
    "            # 3. Time Statistics\n",
    "            'Time_relative': ['max', 'count'],   # max = Duration, count = N_observations\n",
    "    \n",
    "            # 4. Error Statistics (Quality Control)\n",
    "            'Flux_err_corrected': ['mean']\n",
    "        }\n",
    "        \n",
    "        # Aggregation block\n",
    "        agg_df = chunk_df.groupby(['object_id', 'Filter']).agg(aggs)\n",
    "        \n",
    "        # Flatten MultiIndex columns\n",
    "        features_df = agg_df.unstack(level='Filter')\n",
    "        \n",
    "        # Flatten the Hierarchical Column Names\n",
    "        # Current format: (Column, Stat, Filter) -> ('Flux_Corrected', 'mean', 'u')\n",
    "        # Desired format: \"Flux_Corrected_mean_u\"\n",
    "        new_columns = []\n",
    "        for col_name, stat_name, filter_name in features_df.columns:\n",
    "            # Create the combined string\n",
    "            new_name = f\"{col_name}_{stat_name}_{filter_name}\"\n",
    "            new_columns.append(new_name)\n",
    "        \n",
    "        # Assign new column names    \n",
    "        features_df.columns = new_columns\n",
    "        \n",
    "        # Fill NaN counts with 0 (no observations)\n",
    "        count_cols = [col for col in features_df.columns if col.endswith('_count_' + col.split('_')[-1])]\n",
    "        features_df[count_cols] = features_df[count_cols].fillna(0)\n",
    "        \n",
    "        # Leave other NaNs for XGBoost to handle\n",
    "        \n",
    "        # Reset index to bring object_id back as a column\n",
    "        features_df = features_df.reset_index()\n",
    "        \n",
    "        # Merge peak time features\n",
    "        features_df = features_df.merge(peak_data_wide, on='object_id', how='left')\n",
    "        \n",
    "        # Append processed chunk to list\n",
    "        processed_chunks.append(features_df)\n",
    "        \n",
    "        # Clean up\n",
    "        del chunk_df, agg_df, features_df, peak_data, peak_data_wide\n",
    "        gc.collect()\n",
    "        \n",
    "    # Concatenate all processed chunks into a single DataFrame\n",
    "    print(\"Concatenating chunks...\")\n",
    "    final_df = pd.concat(processed_chunks)\n",
    "    \n",
    "    # Merge with log_df to get target labels if training data\n",
    "    if train:\n",
    "        final_df = final_df.merge(log_df[['object_id', 'target', 'Z', 'Z_err']], on='object_id', how='left')\n",
    "    else:\n",
    "        final_df = final_df.merge(log_df[['object_id', 'Z', 'Z_err']], on='object_id', how='left')\n",
    "    \n",
    "    print(\"Processing complete.\")\n",
    "    \n",
    "    # Clean up unnecessary columns\n",
    "    final_df.drop(columns=['Time (MJD)', 'Flux', 'Flux_err', 'Total_extinction'], errors='ignore', inplace=True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1271e9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split_01...\n",
      "Processing split_02...\n",
      "Processing split_03...\n",
      "Processing split_04...\n",
      "Processing split_05...\n",
      "Processing split_06...\n",
      "Processing split_07...\n",
      "Processing split_08...\n",
      "Processing split_09...\n",
      "Processing split_10...\n",
      "Processing split_11...\n",
      "Processing split_12...\n",
      "Processing split_13...\n",
      "Processing split_14...\n",
      "Processing split_15...\n",
      "Processing split_16...\n",
      "Processing split_17...\n",
      "Processing split_18...\n",
      "Processing split_19...\n",
      "Processing split_20...\n",
      "Concatenating chunks...\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Process training data\n",
    "train_features = feature_extraction(RAW_DATA_PATH, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c763bd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>Flux_corrected_mean_g</th>\n",
       "      <th>Flux_corrected_mean_i</th>\n",
       "      <th>Flux_corrected_mean_r</th>\n",
       "      <th>Flux_corrected_mean_u</th>\n",
       "      <th>Flux_corrected_mean_y</th>\n",
       "      <th>Flux_corrected_mean_z</th>\n",
       "      <th>Flux_corrected_max_g</th>\n",
       "      <th>Flux_corrected_max_i</th>\n",
       "      <th>Flux_corrected_max_r</th>\n",
       "      <th>...</th>\n",
       "      <th>Flux_err_corrected_mean_z</th>\n",
       "      <th>Time_to_peak_g</th>\n",
       "      <th>Time_to_peak_i</th>\n",
       "      <th>Time_to_peak_r</th>\n",
       "      <th>Time_to_peak_u</th>\n",
       "      <th>Time_to_peak_y</th>\n",
       "      <th>Time_to_peak_z</th>\n",
       "      <th>target</th>\n",
       "      <th>Z</th>\n",
       "      <th>Z_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dornhoth_fervain_onodrim</td>\n",
       "      <td>-0.541341</td>\n",
       "      <td>2.601577</td>\n",
       "      <td>1.412578</td>\n",
       "      <td>0.950745</td>\n",
       "      <td>-0.458879</td>\n",
       "      <td>1.489760</td>\n",
       "      <td>1.424000</td>\n",
       "      <td>28.277937</td>\n",
       "      <td>13.802700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667111</td>\n",
       "      <td>510.5107</td>\n",
       "      <td>457.6993</td>\n",
       "      <td>466.5012</td>\n",
       "      <td>466.5012</td>\n",
       "      <td>475.3031</td>\n",
       "      <td>457.6993</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0490</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dornhoth_galadh_ylf</td>\n",
       "      <td>0.218875</td>\n",
       "      <td>0.394725</td>\n",
       "      <td>0.317303</td>\n",
       "      <td>0.034930</td>\n",
       "      <td>0.757253</td>\n",
       "      <td>0.573209</td>\n",
       "      <td>1.518313</td>\n",
       "      <td>5.797249</td>\n",
       "      <td>3.027457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579614</td>\n",
       "      <td>1550.8067</td>\n",
       "      <td>1555.9419</td>\n",
       "      <td>1550.8067</td>\n",
       "      <td>729.1873</td>\n",
       "      <td>1558.5094</td>\n",
       "      <td>1561.0770</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4324</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elrim_melethril_thul</td>\n",
       "      <td>3.757406</td>\n",
       "      <td>7.781453</td>\n",
       "      <td>5.462329</td>\n",
       "      <td>0.078012</td>\n",
       "      <td>0.065533</td>\n",
       "      <td>9.287801</td>\n",
       "      <td>8.691179</td>\n",
       "      <td>14.758927</td>\n",
       "      <td>11.552149</td>\n",
       "      <td>...</td>\n",
       "      <td>1.089832</td>\n",
       "      <td>694.1133</td>\n",
       "      <td>733.1571</td>\n",
       "      <td>698.4515</td>\n",
       "      <td>1138.7796</td>\n",
       "      <td>767.8628</td>\n",
       "      <td>733.1571</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4673</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ithil_tobas_rodwen</td>\n",
       "      <td>0.289940</td>\n",
       "      <td>0.455992</td>\n",
       "      <td>0.448044</td>\n",
       "      <td>0.161333</td>\n",
       "      <td>0.348274</td>\n",
       "      <td>0.540509</td>\n",
       "      <td>1.425550</td>\n",
       "      <td>2.269820</td>\n",
       "      <td>1.922080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441839</td>\n",
       "      <td>1119.8792</td>\n",
       "      <td>1119.8792</td>\n",
       "      <td>2463.1615</td>\n",
       "      <td>2781.0811</td>\n",
       "      <td>2735.2549</td>\n",
       "      <td>1105.5585</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mirion_adar_Druadan</td>\n",
       "      <td>0.056120</td>\n",
       "      <td>0.428605</td>\n",
       "      <td>0.242114</td>\n",
       "      <td>-0.018189</td>\n",
       "      <td>0.308948</td>\n",
       "      <td>0.322406</td>\n",
       "      <td>1.798828</td>\n",
       "      <td>6.010829</td>\n",
       "      <td>2.709919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.603441</td>\n",
       "      <td>779.7534</td>\n",
       "      <td>1115.5113</td>\n",
       "      <td>996.2289</td>\n",
       "      <td>775.3356</td>\n",
       "      <td>1201.6597</td>\n",
       "      <td>631.7549</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  object_id  Flux_corrected_mean_g  Flux_corrected_mean_i  \\\n",
       "0  Dornhoth_fervain_onodrim              -0.541341               2.601577   \n",
       "1       Dornhoth_galadh_ylf               0.218875               0.394725   \n",
       "2      Elrim_melethril_thul               3.757406               7.781453   \n",
       "3        Ithil_tobas_rodwen               0.289940               0.455992   \n",
       "4       Mirion_adar_Druadan               0.056120               0.428605   \n",
       "\n",
       "   Flux_corrected_mean_r  Flux_corrected_mean_u  Flux_corrected_mean_y  \\\n",
       "0               1.412578               0.950745              -0.458879   \n",
       "1               0.317303               0.034930               0.757253   \n",
       "2               5.462329               0.078012               0.065533   \n",
       "3               0.448044               0.161333               0.348274   \n",
       "4               0.242114              -0.018189               0.308948   \n",
       "\n",
       "   Flux_corrected_mean_z  Flux_corrected_max_g  Flux_corrected_max_i  \\\n",
       "0               1.489760              1.424000             28.277937   \n",
       "1               0.573209              1.518313              5.797249   \n",
       "2               9.287801              8.691179             14.758927   \n",
       "3               0.540509              1.425550              2.269820   \n",
       "4               0.322406              1.798828              6.010829   \n",
       "\n",
       "   Flux_corrected_max_r  ...  Flux_err_corrected_mean_z  Time_to_peak_g  \\\n",
       "0             13.802700  ...                   0.667111        510.5107   \n",
       "1              3.027457  ...                   0.579614       1550.8067   \n",
       "2             11.552149  ...                   1.089832        694.1133   \n",
       "3              1.922080  ...                   0.441839       1119.8792   \n",
       "4              2.709919  ...                   0.603441        779.7534   \n",
       "\n",
       "   Time_to_peak_i  Time_to_peak_r  Time_to_peak_u  Time_to_peak_y  \\\n",
       "0        457.6993        466.5012        466.5012        475.3031   \n",
       "1       1555.9419       1550.8067        729.1873       1558.5094   \n",
       "2        733.1571        698.4515       1138.7796        767.8628   \n",
       "3       1119.8792       2463.1615       2781.0811       2735.2549   \n",
       "4       1115.5113        996.2289        775.3356       1201.6597   \n",
       "\n",
       "   Time_to_peak_z  target       Z  Z_err  \n",
       "0        457.6993       0  3.0490    NaN  \n",
       "1       1561.0770       0  0.4324    NaN  \n",
       "2        733.1571       0  0.4673    NaN  \n",
       "3       1105.5585       0  0.6946    NaN  \n",
       "4        631.7549       0  0.4161    NaN  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLFinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
