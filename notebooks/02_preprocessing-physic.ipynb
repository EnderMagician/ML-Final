{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82b9284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup & configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- File Paths ---\n",
    "# Using pathlib ensures this works on Windows/Linux/Mac\n",
    "RAW_DATA_PATH = Path(\"../data/raw\")\n",
    "PROCESSED_DATA_PATH = Path(\"../data/processed\")\n",
    "TRAIN_MODE = True # Set to False for test data processing\n",
    "\n",
    "# --- Physics Constants ---\n",
    "# Effective Wavelengths (Angstroms)\n",
    "LAMBDA_EFF = {\n",
    "    'u': 3670, 'g': 4825, 'r': 6261, \n",
    "    'i': 7672, 'z': 9097, 'y': 9600\n",
    "}\n",
    "\n",
    "# Galactic Extinction Coefficients\n",
    "EXTINCTION_COEFFICIENTS = {\n",
    "    'u': 4.81, 'g': 3.64, 'r': 2.70,\n",
    "    'i': 2.06, 'z': 1.58, 'y': 1.31\n",
    "}\n",
    "\n",
    "BANDS = ['u', 'g', 'r', 'i', 'z', 'y']\n",
    "\n",
    "# Redshift Limits (TDEs are usually Z < 1.9)\n",
    "Z_LIMIT_TDE = 1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84653935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic physics functions\n",
    "\n",
    "def calculate_magnitudes(flux, flux_err):\n",
    "    \"\"\"\n",
    "    Converts Flux to Luptitude (arcsinh magnitude).\n",
    "    Handles negative flux naturally.\n",
    "    \"\"\"\n",
    "    # 2.5 * log10(e) approx 1.0857\n",
    "    x = flux / (flux_err + 1e-9)\n",
    "    return -2.5 * np.arcsinh(x)\n",
    "\n",
    "def get_weights(flux_err):\n",
    "    \"\"\"Inverse variance weighting\"\"\"\n",
    "    return 1.0 / (flux_err**2 + 1e-9)\n",
    "\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    \"\"\"\n",
    "    Return the weighted average and standard deviation.\n",
    "    \"\"\"\n",
    "    average = np.average(values, weights=weights)\n",
    "    variance = np.average((values - average)**2, weights=weights)\n",
    "    return average, np.sqrt(variance)\n",
    "\n",
    "def calculate_dust_impact(ebv, band_coeff):\n",
    "    \"\"\"\n",
    "    Calculates transparency (fraction of light that survives dust).\n",
    "    \"\"\"\n",
    "    return 10**(-0.4 * band_coeff * ebv)\n",
    "\n",
    "def calculate_stetson_k(flux, flux_err, band_mean_flux):\n",
    "    \"\"\"\n",
    "    Vectorized calculation of Stetson K residuals (robust kurtosis).\n",
    "    \"\"\"\n",
    "    scaled_residual = (flux - band_mean_flux) / (flux_err + 1e-9)\n",
    "    return np.abs(scaled_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1ee2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional feature calculations\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def calculate_percentile_features(fluxes):\n",
    "    \"\"\"\n",
    "    Captures the distribution shape without curve fitting.\n",
    "    \"\"\"\n",
    "    if len(fluxes) == 0: return {}\n",
    "    \n",
    "    # Percentiles\n",
    "    p = np.percentile(fluxes, [5, 25, 50, 75, 95])\n",
    "    \n",
    "    return {\n",
    "        'flux_min': p[0],\n",
    "        'flux_25': p[1],\n",
    "        'flux_median': p[2],\n",
    "        'flux_75': p[3],\n",
    "        'flux_max': p[4],\n",
    "        'flux_amp': p[4] - p[0], # Amplitude\n",
    "        'flux_ratio_sq': (p[4] / (p[2] + 0.1)) # Peakiness relative to median\n",
    "    }\n",
    "\n",
    "def calculate_time_features(times, fluxes):\n",
    "    \"\"\"\n",
    "    Captures Rise/Fall behavior using raw timestamps.\n",
    "    \"\"\"\n",
    "    if len(times) < 2: return {}\n",
    "    \n",
    "    # Sort by time\n",
    "    idx = np.argsort(times)\n",
    "    t = times[idx]\n",
    "    f = fluxes[idx]\n",
    "    \n",
    "    # Time of Max Flux\n",
    "    idx_max = np.argmax(f)\n",
    "    t_max = t[idx_max]\n",
    "    \n",
    "    # Rise and Fall durations\n",
    "    t_rise = t_max - t[0]\n",
    "    t_fall = t[-1] - t_max\n",
    "    \n",
    "    return {\n",
    "        'time_total': t[-1] - t[0],\n",
    "        'time_rise': t_rise,\n",
    "        'time_fall': t_fall,\n",
    "        'rise_fall_ratio': t_rise / (t_fall + 1e-5) # TDEs have fast rise, slow decay\n",
    "    }\n",
    "\n",
    "def calculate_detected_features(times, fluxes, errors, threshold=5.0):\n",
    "    \"\"\"\n",
    "    Calculates statistics only on 'significant' detections (SNR > 5).\n",
    "    Crucial for distinguishing real events from background noise.\n",
    "    \"\"\"\n",
    "    snr = fluxes / (errors + 1e-9)\n",
    "    mask = snr > threshold\n",
    "    \n",
    "    if not np.any(mask):\n",
    "        return {'detected_duration': 0, 'detected_count': 0}\n",
    "    \n",
    "    t_det = times[mask]\n",
    "    \n",
    "    return {\n",
    "        'detected_duration': t_det.max() - t_det.min(),\n",
    "        'detected_count': len(t_det),\n",
    "        'detected_max_snr': np.max(snr[mask])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f8dfb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing function\n",
    "\n",
    "def process_chunk(chunk_df, log_df):\n",
    "    # 1. Merge Metadata\n",
    "    chunk_df = chunk_df.merge(log_df[['object_id', 'Z', 'EBV']], on=\"object_id\", how=\"left\")\n",
    "    \n",
    "    # 2. Vectorized Corrections\n",
    "    ext_factor = chunk_df[\"Filter\"].map(EXTINCTION_COEFFICIENTS)\n",
    "    chunk_df[\"correction_scale\"] = 10**(0.4 * ext_factor * chunk_df[\"EBV\"])\n",
    "    chunk_df[\"Flux_corr\"] = chunk_df[\"Flux\"] * chunk_df[\"correction_scale\"]\n",
    "    chunk_df[\"Flux_err_corr\"] = chunk_df[\"Flux_err\"] * chunk_df[\"correction_scale\"]\n",
    "    \n",
    "    # --- NEW: Pre-calculate Signal-to-Noise Ratio (SNR) ---\n",
    "    # We add this before aggregation so we can compute statistics on it\n",
    "    chunk_df[\"SNR\"] = chunk_df[\"Flux_corr\"] / (chunk_df[\"Flux_err_corr\"] + 1e-9)\n",
    "\n",
    "    # 3. Aggregations (Vectorized)\n",
    "    aggs = {\n",
    "        'Flux_corr': [\n",
    "            'mean', 'std', 'max', 'min', 'median',  # Basics\n",
    "            skew, kurtosis                          # Shape: TDEs are skewed and peaked\n",
    "        ],\n",
    "        'Flux_err_corr': ['mean'],\n",
    "        'SNR': ['max', 'mean'],                     # Signal Quality: High Max SNR = Real Event\n",
    "        'correction_scale': ['max'],                # Dust penalty\n",
    "        'Time (MJD)': ['count', lambda x: x.max() - x.min()] # Density & Duration (Fast Proxy)\n",
    "    }\n",
    "    \n",
    "    agg_df = chunk_df.groupby(['object_id', 'Filter']).agg(aggs)\n",
    "    \n",
    "    # Flatten Columns & Rename for clarity\n",
    "    new_cols = []\n",
    "    for c in agg_df.columns:\n",
    "        col_name = f\"{c[0]}_{c[1]}\"\n",
    "        col_name = col_name.replace('skew', 'skewness').replace('kurtosis', 'kurt')\n",
    "        col_name = col_name.replace('<lambda_0>', 'duration_proxy') # Rename the duration lambda\n",
    "        new_cols.append(col_name)\n",
    "    \n",
    "    agg_df.columns = new_cols\n",
    "    \n",
    "    # Unstack & Clean\n",
    "    features = agg_df.unstack(level='Filter')\n",
    "    features.columns = [f\"{c[0]}_{c[1]}\".replace('Band ', '').replace(' ', '_') for c in features.columns]\n",
    "    features = features.reset_index()\n",
    "    \n",
    "    # 4. Feature Loop (Robust Logic)\n",
    "    # Kept as is for the specific percentile logic which is harder to aggregate\n",
    "    robust_features = []\n",
    "    \n",
    "    for obj_id, group in chunk_df.groupby('object_id'):\n",
    "        obj_feats = {'object_id': obj_id}\n",
    "        z = group['Z'].iloc[0]\n",
    "        dist_mod = (5 * np.log10(z + 1e-5) + 42) if (pd.notna(z) and z > 0) else 0\n",
    "\n",
    "        # --- PER BAND METRICS ---\n",
    "        for band in ['g', 'r', 'i']:\n",
    "            band_data = group[group['Filter'] == f'Band {band}']\n",
    "            if len(band_data) < 3: continue \n",
    "            \n",
    "            t = band_data['Time (MJD)'].values\n",
    "            f = band_data['Flux_corr'].values\n",
    "            e = band_data['Flux_err_corr'].values\n",
    "            \n",
    "            # A. Shape (Percentiles)\n",
    "            p_feats = calculate_percentile_features(f)\n",
    "            for k, v in p_feats.items():\n",
    "                obj_feats[f'{k}_{band}'] = v\n",
    "            \n",
    "            # B. Time (Rise/Fall)\n",
    "            t_feats = calculate_time_features(t, f)\n",
    "            for k, v in t_feats.items():\n",
    "                obj_feats[f'{k}_{band}'] = v\n",
    "                \n",
    "            # C. Detection (Signal Strength)\n",
    "            d_feats = calculate_detected_features(t, f, e)\n",
    "            for k, v in d_feats.items():\n",
    "                obj_feats[f'{k}_{band}'] = v\n",
    "\n",
    "            # D. Absolute Magnitude\n",
    "            if pd.notna(z) and z > 0:\n",
    "                obj_feats[f'abs_mag_max_{band}'] = -2.5 * np.arcsinh(p_feats.get('flux_max', 1)) - dist_mod\n",
    "        \n",
    "        # --- GLOBAL COLORS ---\n",
    "        if 'flux_max_g' in obj_feats and 'flux_max_r' in obj_feats:\n",
    "            obj_feats['peak_color_g_r'] = obj_feats['flux_max_g'] / (obj_feats['flux_max_r'] + 1e-5)\n",
    "            \n",
    "        obj_feats['low_Z'] = 1 if (pd.notna(z) and z < Z_LIMIT_TDE) else 0\n",
    "            \n",
    "        robust_features.append(obj_feats)\n",
    "\n",
    "    # Merge\n",
    "    df_robust = pd.DataFrame(robust_features)\n",
    "    final_chunk = features.merge(df_robust, on='object_id', how='left')\n",
    "    \n",
    "    return final_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43dd478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Processing... Mode: TRAIN\n",
      "Processing split_01...\n",
      "Processing split_02...\n",
      "Processing split_03...\n",
      "Processing split_04...\n",
      "Processing split_05...\n",
      "Processing split_06...\n",
      "Processing split_07...\n",
      "Processing split_08...\n",
      "Processing split_09...\n",
      "Processing split_10...\n",
      "Processing split_11...\n",
      "Processing split_12...\n",
      "Processing split_13...\n",
      "Processing split_14...\n",
      "Processing split_15...\n",
      "Processing split_16...\n",
      "Processing split_17...\n",
      "Processing split_18...\n",
      "Processing split_19...\n",
      "Processing split_20...\n",
      "Chunk processing complete. Shape: (3043, 80)\n"
     ]
    }
   ],
   "source": [
    "# Execution Loop\n",
    "processed_chunks = []\n",
    "\n",
    "# Load Metadata\n",
    "log_df = pd.read_csv(RAW_DATA_PATH / \"train_log.csv\" if TRAIN_MODE else RAW_DATA_PATH / \"test_log.csv\")\n",
    "\n",
    "print(f\"Starting Processing... Mode: {'TRAIN' if TRAIN_MODE else 'TEST'}\")\n",
    "\n",
    "# Loop through splits\n",
    "for i in range(1, 21): # Assuming 20 splits\n",
    "    folder = f\"split_{i:02d}\"\n",
    "    \n",
    "    # Construct path\n",
    "    prefix = \"train_\" if TRAIN_MODE else \"test_\"\n",
    "    file_path = RAW_DATA_PATH / folder / (prefix + \"full_lightcurves.csv\")\n",
    "    \n",
    "    if not file_path.exists(): continue\n",
    "    \n",
    "    print(f\"Processing {folder}...\")\n",
    "    chunk = pd.read_csv(file_path)\n",
    "    \n",
    "    # Run Engine\n",
    "    features = process_chunk(chunk, log_df)\n",
    "    processed_chunks.append(features)\n",
    "    \n",
    "    # Memory Management\n",
    "    del chunk, features\n",
    "    gc.collect()\n",
    "\n",
    "# Concatenate results\n",
    "full_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "print(f\"Chunk processing complete. Shape: {full_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cbbd669d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 intermediate calculation columns...\n",
      "Engineering Complete.\n",
      "Final Dataset Shape: (3043, 86)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>Flux_corr_mean_g</th>\n",
       "      <th>Flux_corr_mean_i</th>\n",
       "      <th>Flux_corr_mean_r</th>\n",
       "      <th>Flux_corr_mean_u</th>\n",
       "      <th>Flux_corr_mean_y</th>\n",
       "      <th>Flux_corr_mean_z</th>\n",
       "      <th>Flux_corr_std_g</th>\n",
       "      <th>Flux_corr_std_i</th>\n",
       "      <th>Flux_corr_std_r</th>\n",
       "      <th>...</th>\n",
       "      <th>Time_(MJD)_duration_proxy_u</th>\n",
       "      <th>Time_(MJD)_duration_proxy_y</th>\n",
       "      <th>Time_(MJD)_duration_proxy_z</th>\n",
       "      <th>low_Z</th>\n",
       "      <th>Z</th>\n",
       "      <th>Z_err</th>\n",
       "      <th>EBV</th>\n",
       "      <th>SpecType</th>\n",
       "      <th>split</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dornhoth_fervain_onodrim</td>\n",
       "      <td>-0.541341</td>\n",
       "      <td>2.601577</td>\n",
       "      <td>1.412578</td>\n",
       "      <td>0.950745</td>\n",
       "      <td>-0.458879</td>\n",
       "      <td>1.489760</td>\n",
       "      <td>0.982121</td>\n",
       "      <td>7.802839</td>\n",
       "      <td>4.255918</td>\n",
       "      <td>...</td>\n",
       "      <td>849.3841</td>\n",
       "      <td>1241.0691</td>\n",
       "      <td>1241.0691</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.110</td>\n",
       "      <td>AGN</td>\n",
       "      <td>split_01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dornhoth_galadh_ylf</td>\n",
       "      <td>0.218875</td>\n",
       "      <td>0.394725</td>\n",
       "      <td>0.317303</td>\n",
       "      <td>0.034930</td>\n",
       "      <td>0.757253</td>\n",
       "      <td>0.573209</td>\n",
       "      <td>0.439899</td>\n",
       "      <td>1.080404</td>\n",
       "      <td>0.759543</td>\n",
       "      <td>...</td>\n",
       "      <td>2246.6157</td>\n",
       "      <td>2362.1560</td>\n",
       "      <td>2297.9670</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.058</td>\n",
       "      <td>SN II</td>\n",
       "      <td>split_01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elrim_melethril_thul</td>\n",
       "      <td>3.757406</td>\n",
       "      <td>7.781453</td>\n",
       "      <td>5.462329</td>\n",
       "      <td>0.078012</td>\n",
       "      <td>0.065533</td>\n",
       "      <td>9.287801</td>\n",
       "      <td>2.995947</td>\n",
       "      <td>8.624490</td>\n",
       "      <td>4.078020</td>\n",
       "      <td>...</td>\n",
       "      <td>498.8939</td>\n",
       "      <td>767.8628</td>\n",
       "      <td>1093.2284</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.577</td>\n",
       "      <td>AGN</td>\n",
       "      <td>split_01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ithil_tobas_rodwen</td>\n",
       "      <td>0.289940</td>\n",
       "      <td>0.455992</td>\n",
       "      <td>0.448044</td>\n",
       "      <td>0.161333</td>\n",
       "      <td>0.348274</td>\n",
       "      <td>0.540509</td>\n",
       "      <td>0.363483</td>\n",
       "      <td>0.643550</td>\n",
       "      <td>0.539392</td>\n",
       "      <td>...</td>\n",
       "      <td>2858.4129</td>\n",
       "      <td>2841.2281</td>\n",
       "      <td>2835.4998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012</td>\n",
       "      <td>AGN</td>\n",
       "      <td>split_01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mirion_adar_Druadan</td>\n",
       "      <td>0.056120</td>\n",
       "      <td>0.428605</td>\n",
       "      <td>0.242114</td>\n",
       "      <td>-0.018189</td>\n",
       "      <td>0.308948</td>\n",
       "      <td>0.322406</td>\n",
       "      <td>0.976711</td>\n",
       "      <td>1.507290</td>\n",
       "      <td>1.246042</td>\n",
       "      <td>...</td>\n",
       "      <td>1859.9219</td>\n",
       "      <td>1809.1164</td>\n",
       "      <td>1901.8916</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.058</td>\n",
       "      <td>AGN</td>\n",
       "      <td>split_01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  object_id  Flux_corr_mean_g  Flux_corr_mean_i  \\\n",
       "0  Dornhoth_fervain_onodrim         -0.541341          2.601577   \n",
       "1       Dornhoth_galadh_ylf          0.218875          0.394725   \n",
       "2      Elrim_melethril_thul          3.757406          7.781453   \n",
       "3        Ithil_tobas_rodwen          0.289940          0.455992   \n",
       "4       Mirion_adar_Druadan          0.056120          0.428605   \n",
       "\n",
       "   Flux_corr_mean_r  Flux_corr_mean_u  Flux_corr_mean_y  Flux_corr_mean_z  \\\n",
       "0          1.412578          0.950745         -0.458879          1.489760   \n",
       "1          0.317303          0.034930          0.757253          0.573209   \n",
       "2          5.462329          0.078012          0.065533          9.287801   \n",
       "3          0.448044          0.161333          0.348274          0.540509   \n",
       "4          0.242114         -0.018189          0.308948          0.322406   \n",
       "\n",
       "   Flux_corr_std_g  Flux_corr_std_i  Flux_corr_std_r  ...  \\\n",
       "0         0.982121         7.802839         4.255918  ...   \n",
       "1         0.439899         1.080404         0.759543  ...   \n",
       "2         2.995947         8.624490         4.078020  ...   \n",
       "3         0.363483         0.643550         0.539392  ...   \n",
       "4         0.976711         1.507290         1.246042  ...   \n",
       "\n",
       "   Time_(MJD)_duration_proxy_u  Time_(MJD)_duration_proxy_y  \\\n",
       "0                     849.3841                    1241.0691   \n",
       "1                    2246.6157                    2362.1560   \n",
       "2                     498.8939                     767.8628   \n",
       "3                    2858.4129                    2841.2281   \n",
       "4                    1859.9219                    1809.1164   \n",
       "\n",
       "   Time_(MJD)_duration_proxy_z  low_Z       Z  Z_err    EBV  SpecType  \\\n",
       "0                    1241.0691      0  3.0490    NaN  0.110       AGN   \n",
       "1                    2297.9670      1  0.4324    NaN  0.058     SN II   \n",
       "2                    1093.2284      1  0.4673    NaN  0.577       AGN   \n",
       "3                    2835.4998      1  0.6946    NaN  0.012       AGN   \n",
       "4                    1901.8916      1  0.4161    NaN  0.058       AGN   \n",
       "\n",
       "      split  target  \n",
       "0  split_01       0  \n",
       "1  split_01       0  \n",
       "2  split_01       0  \n",
       "3  split_01       0  \n",
       "4  split_01       0  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Assembly and Cleanup\n",
    "\n",
    "# 1. Post-Process: Calculate Weighted Means\n",
    "for b in BANDS:\n",
    "    num_col = f\"w_flux_sum_{b}\"\n",
    "    den_col = f\"w_sum_{b}\"\n",
    "    \n",
    "    if num_col in full_df.columns and den_col in full_df.columns:\n",
    "        # Weighted Mean Flux\n",
    "        full_df[f\"flux_w_mean_{b}\"] = full_df[num_col] / (full_df[den_col] + 1e-9)\n",
    "        # Weighted SNR (approximate)\n",
    "        full_df[f\"snr_weight_{b}\"] = np.sqrt(full_df[den_col])\n",
    "\n",
    "# 2. Post-Process: Calculate Colors\n",
    "# Color = u-g, g-r, etc.\n",
    "for b1, b2 in zip(BANDS[:-1], BANDS[1:]):\n",
    "    col1 = f\"flux_w_mean_{b1}\" # Using weighted mean flux\n",
    "    col2 = f\"flux_w_mean_{b2}\"\n",
    "    \n",
    "    if col1 in full_df.columns and col2 in full_df.columns:\n",
    "        # Simple flux difference or ratio proxy\n",
    "        full_df[f\"color_diff_{b1}_{b2}\"] = full_df[col1] - full_df[col2]\n",
    "\n",
    "# 3. Post-Process: TDE Specifics\n",
    "# \"Confident Blue Flux\": High U-band flux that survived dust correction\n",
    "if 'flux_w_mean_u' in full_df.columns and 'u_band_transparency' in full_df.columns:\n",
    "    full_df['confident_blue_flux'] = full_df['flux_w_mean_u'] * full_df['u_band_transparency']\n",
    "\n",
    "# 4. Cleanup: Drop Intermediate/Error Columns\n",
    "cols_to_drop = [c for c in full_df.columns if c.startswith('w_flux_sum') or c.startswith('w_sum')]\n",
    "print(f\"Dropping {len(cols_to_drop)} intermediate calculation columns...\")\n",
    "full_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# 5. Final Merge with Target\n",
    "final_dataset = full_df.merge(log_df, on='object_id', how='left')\n",
    "\n",
    "# Drop non-feature columns\n",
    "final_dataset.drop(columns=['English Translation'], errors='ignore', inplace=True)\n",
    "\n",
    "print(\"Engineering Complete.\")\n",
    "print(\"Final Dataset Shape:\", final_dataset.shape)\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d1d0d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Found 26 highly correlated pairs (Threshold: 0.95)\n",
      "üìâ Suggest dropping 15 columns to reduce noise.\n",
      "\n",
      " Correlated Feature Pairs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>correction_scale_max_y</td>\n",
       "      <td>correction_scale_max_z</td>\n",
       "      <td>0.999454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>correction_scale_max_i</td>\n",
       "      <td>correction_scale_max_z</td>\n",
       "      <td>0.998196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>correction_scale_max_i</td>\n",
       "      <td>correction_scale_max_r</td>\n",
       "      <td>0.996460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>correction_scale_max_i</td>\n",
       "      <td>correction_scale_max_y</td>\n",
       "      <td>0.995571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>correction_scale_max_g</td>\n",
       "      <td>correction_scale_max_r</td>\n",
       "      <td>0.991678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>correction_scale_max_r</td>\n",
       "      <td>correction_scale_max_z</td>\n",
       "      <td>0.989628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>correction_scale_max_y</td>\n",
       "      <td>EBV</td>\n",
       "      <td>0.989543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>correction_scale_max_g</td>\n",
       "      <td>correction_scale_max_u</td>\n",
       "      <td>0.986763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>correction_scale_max_z</td>\n",
       "      <td>EBV</td>\n",
       "      <td>0.984800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>correction_scale_max_r</td>\n",
       "      <td>correction_scale_max_y</td>\n",
       "      <td>0.983937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>correction_scale_max_g</td>\n",
       "      <td>correction_scale_max_i</td>\n",
       "      <td>0.977402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Flux_corr_mean_y</td>\n",
       "      <td>SNR_mean_y</td>\n",
       "      <td>0.973908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Flux_corr_mean_z</td>\n",
       "      <td>SNR_mean_z</td>\n",
       "      <td>0.973141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>correction_scale_max_i</td>\n",
       "      <td>EBV</td>\n",
       "      <td>0.972682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>correction_scale_max_g</td>\n",
       "      <td>correction_scale_max_z</td>\n",
       "      <td>0.963068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Flux_err_corr_mean_u</td>\n",
       "      <td>correction_scale_max_u</td>\n",
       "      <td>0.962771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Flux_corr_mean_r</td>\n",
       "      <td>SNR_mean_r</td>\n",
       "      <td>0.958655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Time_(MJD)_duration_proxy_i</td>\n",
       "      <td>Time_(MJD)_duration_proxy_r</td>\n",
       "      <td>0.958616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Time_(MJD)_count_g</td>\n",
       "      <td>Time_(MJD)_count_u</td>\n",
       "      <td>0.957902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>correction_scale_max_r</td>\n",
       "      <td>correction_scale_max_u</td>\n",
       "      <td>0.957114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Flux_corr_mean_i</td>\n",
       "      <td>SNR_mean_i</td>\n",
       "      <td>0.954717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Time_(MJD)_count_i</td>\n",
       "      <td>Time_(MJD)_count_r</td>\n",
       "      <td>0.953855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Flux_corr_mean_i</td>\n",
       "      <td>Flux_corr_mean_z</td>\n",
       "      <td>0.953705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Time_(MJD)_duration_proxy_i</td>\n",
       "      <td>Time_(MJD)_duration_proxy_z</td>\n",
       "      <td>0.952776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>correction_scale_max_g</td>\n",
       "      <td>correction_scale_max_y</td>\n",
       "      <td>0.952576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>correction_scale_max_r</td>\n",
       "      <td>EBV</td>\n",
       "      <td>0.950001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature 1                    Feature 2  Correlation\n",
       "0        correction_scale_max_y       correction_scale_max_z     0.999454\n",
       "1        correction_scale_max_i       correction_scale_max_z     0.998196\n",
       "2        correction_scale_max_i       correction_scale_max_r     0.996460\n",
       "3        correction_scale_max_i       correction_scale_max_y     0.995571\n",
       "4        correction_scale_max_g       correction_scale_max_r     0.991678\n",
       "5        correction_scale_max_r       correction_scale_max_z     0.989628\n",
       "6        correction_scale_max_y                          EBV     0.989543\n",
       "7        correction_scale_max_g       correction_scale_max_u     0.986763\n",
       "8        correction_scale_max_z                          EBV     0.984800\n",
       "9        correction_scale_max_r       correction_scale_max_y     0.983937\n",
       "10       correction_scale_max_g       correction_scale_max_i     0.977402\n",
       "11             Flux_corr_mean_y                   SNR_mean_y     0.973908\n",
       "12             Flux_corr_mean_z                   SNR_mean_z     0.973141\n",
       "13       correction_scale_max_i                          EBV     0.972682\n",
       "14       correction_scale_max_g       correction_scale_max_z     0.963068\n",
       "15         Flux_err_corr_mean_u       correction_scale_max_u     0.962771\n",
       "16             Flux_corr_mean_r                   SNR_mean_r     0.958655\n",
       "17  Time_(MJD)_duration_proxy_i  Time_(MJD)_duration_proxy_r     0.958616\n",
       "18           Time_(MJD)_count_g           Time_(MJD)_count_u     0.957902\n",
       "19       correction_scale_max_r       correction_scale_max_u     0.957114\n",
       "20             Flux_corr_mean_i                   SNR_mean_i     0.954717\n",
       "21           Time_(MJD)_count_i           Time_(MJD)_count_r     0.953855\n",
       "22             Flux_corr_mean_i             Flux_corr_mean_z     0.953705\n",
       "23  Time_(MJD)_duration_proxy_i  Time_(MJD)_duration_proxy_z     0.952776\n",
       "24       correction_scale_max_g       correction_scale_max_y     0.952576\n",
       "25       correction_scale_max_r                          EBV     0.950001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === CELL: FEATURE CORRELATION CHECK ===\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_high_correlations(df, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Identifies feature pairs with correlation above 'threshold'.\n",
    "    Returns a dataframe of the pairs and a list of suggested columns to drop.\n",
    "    \"\"\"\n",
    "    # Select only numeric columns\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calculate Correlation Matrix (Absolute value: -0.99 is just as redundant as 0.99)\n",
    "    corr_matrix = numeric_df.corr().abs()\n",
    "    \n",
    "    # Select upper triangle of correlation matrix to avoid duplicates (A-B vs B-A) and self-correlation (A-A)\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    # Find features with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    # Create a detailed report\n",
    "    record = []\n",
    "    for col in upper.columns:\n",
    "        # Get rows where correlation is high\n",
    "        high_corr = upper.index[upper[col] > threshold].tolist()\n",
    "        for row in high_corr:\n",
    "            record.append({\n",
    "                'Feature 1': row,\n",
    "                'Feature 2': col,\n",
    "                'Correlation': upper.loc[row, col]\n",
    "            })\n",
    "            \n",
    "    report_df = pd.DataFrame(record)\n",
    "    \n",
    "    if not report_df.empty:\n",
    "        report_df = report_df.sort_values(by='Correlation', ascending=False).reset_index(drop=True)\n",
    "        print(f\"‚ö†Ô∏è Found {len(report_df)} highly correlated pairs (Threshold: {threshold})\")\n",
    "        print(f\"üìâ Suggest dropping {len(to_drop)} columns to reduce noise.\")\n",
    "    else:\n",
    "        print(\"‚úÖ No highly correlated pairs found.\")\n",
    "        \n",
    "    return report_df, to_drop\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# Exclude 'target' and 'object_id' from this check\n",
    "features_to_check = final_dataset.drop(columns=['target', 'object_id', 'Z_err'], errors='ignore')\n",
    "\n",
    "corr_report, suggested_drops = analyze_high_correlations(features_to_check, threshold=0.95)\n",
    "\n",
    "# Display Redundant Pairs\n",
    "print(\"\\n Correlated Feature Pairs:\")\n",
    "display(corr_report)\n",
    "\n",
    "# Visual Heatmap (Only if < 50 features, otherwise it's unreadable)\n",
    "if len(features_to_check.columns) < 50:\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(features_to_check.corr(), cmap='coolwarm', center=0, annot=False)\n",
    "    plt.title(\"Feature Correlation Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7456c1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to ..\\data\\processed\\train_features.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save Processed Data\n",
    "output_file = PROCESSED_DATA_PATH / (\"train_features.parquet\" if TRAIN_MODE else \"test_features.parquet\")\n",
    "final_dataset.to_parquet(output_file, index=False)\n",
    "print(f\"Saved processed data to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLFinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
