{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5756acb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Master Metadata...\n",
      "Metadata loaded. Total objects: 3043\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc # Garbage Collector\n",
    "import os\n",
    "import sys\n",
    "# add src directory to path for module imports\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from tqdm import tqdm # Progress bar\n",
    "\n",
    "from src import preprocess\n",
    "\n",
    "# Configuration\n",
    "RAW_DIR = '../data/raw'\n",
    "PROCESSED_DIR = '../data/processed'\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# Load Master Metadata\n",
    "print(\"Loading Master Metadata...\")\n",
    "meta_df = pd.read_csv(f'{RAW_DIR}/train_log.csv') \n",
    "\n",
    "print(f\"Metadata loaded. Total objects: {len(meta_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25083d",
   "metadata": {},
   "source": [
    "As the data is massive, we'll perform a loop to extract features from each split and then delete them to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "713c1917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split_01...\n",
      "Processing split_02...\n",
      "Processing split_03...\n",
      "Processing split_04...\n",
      "Processing split_05...\n",
      "Processing split_06...\n",
      "Processing split_07...\n",
      "Processing split_08...\n",
      "Processing split_09...\n",
      "Processing split_10...\n",
      "Processing split_11...\n",
      "Processing split_12...\n",
      "Processing split_13...\n",
      "Processing split_14...\n",
      "Processing split_15...\n",
      "Processing split_16...\n",
      "Processing split_17...\n",
      "Processing split_18...\n",
      "Processing split_19...\n",
      "Processing split_20...\n",
      "All splits processed.\n"
     ]
    }
   ],
   "source": [
    "# Storage for the extracted features from all splits\n",
    "all_features_batches = []\n",
    "\n",
    "# Loop through splits 01 to 20\n",
    "for i in range(1, 21): \n",
    "    split_name = f\"split_{i:02d}\" # Formats to 'split_01', 'split_02'\n",
    "    lc_path = f\"{RAW_DIR}/{split_name}/train_full_lightcurves.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(lc_path):\n",
    "        print(f\"Skipping {split_name} (File not found)\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing {split_name}...\")\n",
    "    \n",
    "    # 1. Load Lightcurves for this split\n",
    "    chunk_lc = pd.read_csv(lc_path)\n",
    "    \n",
    "    # 2. Filter Metadata to match ONLY these objects\n",
    "    chunk_meta = meta_df[meta_df['object_id'].isin(chunk_lc['object_id'])].copy()\n",
    "    \n",
    "    # 3. Clean Data (Physics)\n",
    "    # This applies the Distance and Dust correction\n",
    "    _, clean_lc = preprocess.clean_data(chunk_meta, chunk_lc)\n",
    "    \n",
    "    # 4. Extract Features (Stats)\n",
    "    # This shrinks millions of rows -> 1 row per object\n",
    "    chunk_features = preprocess.extract_features(clean_lc)\n",
    "    \n",
    "    # 5. Store Result\n",
    "    all_features_batches.append(chunk_features)\n",
    "    \n",
    "    # 6. Memory Cleanup\n",
    "    del chunk_lc, clean_lc, chunk_features, chunk_meta\n",
    "    gc.collect() # Force RAM release\n",
    "\n",
    "print(\"All splits processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae0bc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining all batches...\n",
      "Final Data Shape: (3043, 8)\n",
      "                  object_id  peak_flux  rise_slope  fall_slope  color_u_z  \\\n",
      "0  Dornhoth_fervain_onodrim  10.311690    0.021273   -0.009711   5.511201   \n",
      "1       Dornhoth_galadh_ylf   7.864956    0.000756   -0.003425  -1.381394   \n",
      "2      Elrim_melethril_thul   8.025164    0.004992    0.002015  -3.384872   \n",
      "3        Ithil_tobas_rodwen   8.010233    0.001751    0.000975  -0.814407   \n",
      "4       Mirion_adar_Druadan   7.515978    0.004017   -0.002686  -0.822449   \n",
      "\n",
      "   target       Z    EBV  \n",
      "0       0  3.0490  0.110  \n",
      "1       0  0.4324  0.058  \n",
      "2       0  0.4673  0.577  \n",
      "3       0  0.6946  0.012  \n",
      "4       0  0.4161  0.058  \n",
      "Successfully saved train.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining all batches...\")\n",
    "# 1. Concatenate all feature batches\n",
    "full_features = pd.concat(all_features_batches, ignore_index=True)\n",
    "\n",
    "# 2. Merge with Labels (Target) and Z\n",
    "# We grab 'target', 'Z', and 'EBV' from the original metadata\n",
    "# This creates the final \"X\" and \"y\" matrix for training\n",
    "final_train_data = full_features.merge(\n",
    "    meta_df[['object_id', 'target', 'Z', 'EBV']], \n",
    "    on='object_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3. Inspect\n",
    "print(f\"Final Data Shape: {final_train_data.shape}\")\n",
    "print(final_train_data.head())\n",
    "\n",
    "# 4. Save\n",
    "# Parquet is 10x faster and smaller than CSV for this data\n",
    "final_train_data.to_parquet(f'{PROCESSED_DIR}/train.parquet')\n",
    "print(\"Successfully saved train.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b9d7d0",
   "metadata": {},
   "source": [
    "We'll do the same to test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "510b1ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Master Metadata...\n",
      "Metadata loaded. Total objects: 7135\n"
     ]
    }
   ],
   "source": [
    "# Load Master Metadata\n",
    "print(\"Loading Master Metadata...\")\n",
    "meta_df = pd.read_csv(f'{RAW_DIR}/test_log.csv') \n",
    "\n",
    "print(f\"Metadata loaded. Total objects: {len(meta_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457970fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split_01...\n",
      "Processing split_02...\n",
      "Processing split_03...\n",
      "Processing split_04...\n",
      "Processing split_05...\n",
      "Processing split_06...\n",
      "Processing split_07...\n",
      "Processing split_08...\n",
      "Processing split_09...\n",
      "Processing split_10...\n",
      "Processing split_11...\n",
      "Processing split_12...\n",
      "Processing split_13...\n",
      "Processing split_14...\n",
      "Processing split_15...\n",
      "Processing split_16...\n",
      "Processing split_17...\n",
      "Processing split_18...\n",
      "Processing split_19...\n",
      "Processing split_20...\n",
      "All splits processed.\n"
     ]
    }
   ],
   "source": [
    "# Storage for the extracted features from all splits\n",
    "all_features_batches = []\n",
    "\n",
    "# Loop through splits 01 to 20\n",
    "for i in range(1, 21): \n",
    "    split_name = f\"split_{i:02d}\" # Formats to 'split_01', 'split_02'\n",
    "    lc_path = f\"{RAW_DIR}/{split_name}/test_full_lightcurves.csv\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(lc_path):\n",
    "        print(f\"Skipping {split_name} (File not found)\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing {split_name}...\")\n",
    "    \n",
    "    # 1. Load Lightcurves for this split\n",
    "    chunk_lc = pd.read_csv(lc_path)\n",
    "    \n",
    "    # 2. Filter Metadata to match ONLY these objects\n",
    "    chunk_meta = meta_df[meta_df['object_id'].isin(chunk_lc['object_id'])].copy()\n",
    "    \n",
    "    # 3. Clean Data (Physics)\n",
    "    # This applies the Distance and Dust correction\n",
    "    _, clean_lc = preprocess.clean_data(chunk_meta, chunk_lc)\n",
    "    \n",
    "    # 4. Extract Features (Stats)\n",
    "    # This shrinks millions of rows -> 1 row per object\n",
    "    chunk_features = preprocess.extract_features(clean_lc)\n",
    "    \n",
    "    # 5. Store Result\n",
    "    all_features_batches.append(chunk_features)\n",
    "    \n",
    "    # 6. Memory Cleanup\n",
    "    del chunk_lc, clean_lc, chunk_features, chunk_meta\n",
    "    gc.collect() # Force RAM release\n",
    "\n",
    "print(\"All splits processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8873cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining all batches...\n",
      "Final Data Shape: (7135, 7)\n",
      "                   object_id  peak_flux  rise_slope  fall_slope  color_u_z  \\\n",
      "0            Elrim_sador_hun   7.038654    0.001275   -0.004790  -0.540200   \n",
      "1   Eluwaith_Mithrim_nothrim   8.476333    0.005925   -0.006400   0.423939   \n",
      "2         Eru_heledir_archam   7.947598    0.006409   -0.000427  -1.535087   \n",
      "3          Gonhir_anann_fuin   7.845152    0.002041   -0.000994  -1.182255   \n",
      "4  Gwathuirim_eilian_fervain   7.769473   -0.000265   -0.017058   0.772347   \n",
      "\n",
      "        Z    EBV  \n",
      "0  0.3003  0.030  \n",
      "1  0.5393  0.610  \n",
      "2  0.7282  0.058  \n",
      "3  0.6026  0.070  \n",
      "4  0.8305  0.038  \n",
      "Successfully saved test.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining all batches...\")\n",
    "# 1. Concatenate all feature batches\n",
    "full_features = pd.concat(all_features_batches, ignore_index=True)\n",
    "\n",
    "# 2. Merge with Labels (Target) and Z\n",
    "# We grab 'target', 'Z', and 'EBV' from the original metadata\n",
    "# This creates the final \"X\" and \"y\" matrix for training\n",
    "final_train_data = full_features.merge(\n",
    "    meta_df[['object_id', 'Z', 'EBV']], \n",
    "    on='object_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3. Inspect\n",
    "print(f\"Final Data Shape: {final_train_data.shape}\")\n",
    "print(final_train_data.head())\n",
    "\n",
    "# 4. Save\n",
    "# Parquet is 10x faster and smaller than CSV for this data\n",
    "final_train_data.to_parquet(f'{PROCESSED_DIR}/test.parquet')\n",
    "print(\"Successfully saved test.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLFinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
