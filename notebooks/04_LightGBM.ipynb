{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# LightGBM Model for TDE Classification\n",
                "\n",
                "This notebook trains a LightGBM model with Optuna hyperparameter tuning.\n",
                "\n",
                "**Key Features:**\n",
                "- Uses the same cross-validation folds as all other models\n",
                "- Handles class imbalance with `scale_pos_weight`\n",
                "- Saves OOF and test predictions for ensemble\n",
                "- Generates individual model submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import lightgbm as lgb\n",
                "import optuna\n",
                "from sklearn.metrics import precision_recall_curve\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "optuna.logging.set_verbosity(optuna.logging.WARNING)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "N_OPTUNA_TRIALS = 100\n",
                "RANDOM_STATE = 15\n",
                "MODEL_NAME = 'lgb'\n",
                "\n",
                "# Paths\n",
                "DATA_DIR = os.path.join('..', 'data', 'processed')\n",
                "MODEL_DIR = os.path.join('..', 'models')\n",
                "SUBMISSION_DIR = os.path.join('..', 'submissions')\n",
                "TRAIN_PATH = os.path.join(DATA_DIR, 'further_train_features.parquet')\n",
                "TEST_PATH = os.path.join(DATA_DIR, 'further_test_features.parquet')\n",
                "FOLDS_PATH = os.path.join(DATA_DIR, 'train_folds.csv')\n",
                "\n",
                "os.makedirs(MODEL_DIR, exist_ok=True)\n",
                "os.makedirs(SUBMISSION_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "print(\"Loading data...\")\n",
                "train = pd.read_parquet(TRAIN_PATH)\n",
                "test = pd.read_parquet(TEST_PATH)\n",
                "folds = pd.read_csv(FOLDS_PATH)\n",
                "\n",
                "# Merge folds with training data\n",
                "train = train.merge(folds[['object_id', 'kfold']], on='object_id', how='left')\n",
                "\n",
                "print(f\"Train shape: {train.shape}\")\n",
                "print(f\"Test shape: {test.shape}\")\n",
                "print(f\"Class distribution: {train['target'].value_counts().to_dict()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "prepare_features",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features\n",
                "drop_cols = ['object_id', 'target', 'split', 'SpecType', 'kfold']\n",
                "feature_cols = [c for c in train.columns if c not in drop_cols]\n",
                "\n",
                "# Clean column names for LightGBM\n",
                "def clean_cols(cols):\n",
                "    return [re.sub(r'[^A-Za-z0-9_]+', '_', str(c)) for c in cols]\n",
                "\n",
                "X = train[feature_cols].copy()\n",
                "X.columns = clean_cols(X.columns)\n",
                "y = train['target']\n",
                "kfold = train['kfold']\n",
                "\n",
                "X_test = test[feature_cols].copy()\n",
                "X_test.columns = clean_cols(X_test.columns)\n",
                "object_ids_test = test['object_id']\n",
                "\n",
                "# Calculate scale_pos_weight for imbalance\n",
                "scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
                "print(f\"Feature count: {len(feature_cols)}\")\n",
                "print(f\"scale_pos_weight: {scale_pos_weight:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "optuna_objective",
            "metadata": {},
            "outputs": [],
            "source": [
                "def objective(trial):\n",
                "    params = {\n",
                "        'objective': 'binary',\n",
                "        'metric': 'average_precision',\n",
                "        'n_estimators': 1000,\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15),\n",
                "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
                "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
                "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
                "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
                "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.95),\n",
                "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
                "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
                "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 10, 40),\n",
                "        'n_jobs': -1,\n",
                "        'random_state': RANDOM_STATE,\n",
                "        'verbosity': -1\n",
                "    }\n",
                "    \n",
                "    # Global OOF Evaluation: Initialize OOF predictions array\n",
                "    oof_preds = np.zeros(len(y))\n",
                "    \n",
                "    for fold in range(5):\n",
                "        train_idx = kfold != fold\n",
                "        val_idx = kfold == fold\n",
                "        \n",
                "        X_tr, X_val = X[train_idx], X[val_idx]\n",
                "        y_tr, y_val = y[train_idx], y[val_idx]\n",
                "        \n",
                "        clf = lgb.LGBMClassifier(**params)\n",
                "        callbacks = [lgb.early_stopping(50, verbose=False)]\n",
                "        clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=callbacks)\n",
                "        \n",
                "        # Store predictions globally (do NOT calculate F1 here)\n",
                "        preds = clf.predict_proba(X_val)[:, 1]\n",
                "        oof_preds[val_idx] = preds\n",
                "    \n",
                "    # Calculate Global Metric (outside loop)\n",
                "    # Optimize threshold on the full dataset\n",
                "    prec, rec, thresholds = precision_recall_curve(y, oof_preds)\n",
                "    f1_scores = 2 * (prec * rec) / (prec + rec + 1e-9)\n",
                "    best_f1 = np.max(f1_scores)\n",
                "    \n",
                "    return best_f1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run_optuna",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Optuna optimization\n",
                "print(f\"Running Optuna with {N_OPTUNA_TRIALS} trials...\")\n",
                "study = optuna.create_study(direction='maximize')\n",
                "study.optimize(objective, n_trials=N_OPTUNA_TRIALS, show_progress_bar=True)\n",
                "\n",
                "print(f\"\\nBest F1 Score: {study.best_value:.4f}\")\n",
                "print(f\"Best params: {study.best_params}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_final",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train final model with best params\n",
                "print(\"\\nTraining final model with best params...\")\n",
                "\n",
                "best_params = study.best_params.copy()\n",
                "best_params.update({\n",
                "    'objective': 'binary',\n",
                "    'metric': 'average_precision',\n",
                "    'n_estimators': 3000,\n",
                "    'n_jobs': -1,\n",
                "    'random_state': RANDOM_STATE,\n",
                "    'verbosity': -1\n",
                "})\n",
                "\n",
                "oof_preds = np.zeros(len(y))\n",
                "test_preds = np.zeros(len(X_test))\n",
                "\n",
                "for fold in range(5):\n",
                "    train_idx = kfold != fold\n",
                "    val_idx = kfold == fold\n",
                "    \n",
                "    X_tr, X_val = X[train_idx], X[val_idx]\n",
                "    y_tr, y_val = y[train_idx], y[val_idx]\n",
                "    \n",
                "    clf = lgb.LGBMClassifier(**best_params)\n",
                "    callbacks = [lgb.early_stopping(150, verbose=False)]\n",
                "    clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=callbacks)\n",
                "    \n",
                "    oof_preds[val_idx] = clf.predict_proba(X_val)[:, 1]\n",
                "    test_preds += clf.predict_proba(X_test)[:, 1] / 5\n",
                "    \n",
                "    print(f\"Fold {fold} complete.\")\n",
                "\n",
                "# Calculate final OOF F1\n",
                "prec, rec, thresh = precision_recall_curve(y, oof_preds)\n",
                "f1 = 2 * (prec[:-1] * rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-9)\n",
                "best_thresh = thresh[np.argmax(f1)]\n",
                "print(f\"\\nOOF F1 Score: {np.max(f1):.4f} at threshold {best_thresh:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save_predictions",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save predictions\n",
                "oof_df = pd.DataFrame({\n",
                "    'object_id': train['object_id'],\n",
                "    'target': y,\n",
                "    f'pred_{MODEL_NAME}': oof_preds\n",
                "})\n",
                "oof_df.to_csv(os.path.join(MODEL_DIR, f'oof_{MODEL_NAME}.csv'), index=False)\n",
                "\n",
                "test_df = pd.DataFrame({\n",
                "    'object_id': object_ids_test,\n",
                "    f'pred_{MODEL_NAME}': test_preds\n",
                "})\n",
                "test_df.to_csv(os.path.join(MODEL_DIR, f'preds_{MODEL_NAME}.csv'), index=False)\n",
                "\n",
                "print(f\"\\nSaved OOF predictions to: models/oof_{MODEL_NAME}.csv\")\n",
                "print(f\"Saved test predictions to: models/preds_{MODEL_NAME}.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "create_submission",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create submission file for this model\n",
                "# Apply optimal threshold from OOF\n",
                "test_binary = (test_preds >= best_thresh).astype(int)\n",
                "\n",
                "# Create submission dataframe\n",
                "submission = pd.DataFrame({\n",
                "    'object_id': object_ids_test,\n",
                "    'target': test_binary\n",
                "})\n",
                "\n",
                "submission_path = os.path.join(SUBMISSION_DIR, f'submission_{MODEL_NAME}.csv')\n",
                "submission.to_csv(submission_path, index=False)\n",
                "\n",
                "print(f\"\\n=== {MODEL_NAME.upper()} Submission ===\")\n",
                "print(f\"Threshold: {best_thresh:.4f}\")\n",
                "print(f\"Prediction distribution: {np.bincount(test_binary)}\")\n",
                "print(f\"Positive rate: {test_binary.mean():.4f}\")\n",
                "print(f\"Saved to: {submission_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "MLFinal",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}