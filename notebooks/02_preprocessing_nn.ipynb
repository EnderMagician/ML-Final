{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Neural Network Preprocessing\n",
                "\n",
                "This notebook prepares data specifically for Neural Network models. While tree-based models (XGBoost, LightGBM, CatBoost) can handle raw data, Neural Networks require:\n",
                "- Feature engineering (NaN count)\n",
                "- Imputation (median fill)\n",
                "- Scaling (StandardScaler)\n",
                "\n",
                "**Important**: The scaler is fit on training data only, then used to transform both train and test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import joblib\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "TRAIN_MODE = False  # Set to False when processing test data\n",
                "\n",
                "# Paths\n",
                "DATA_DIR = os.path.join('..', 'data', 'processed')\n",
                "TRAIN_INPUT = os.path.join(DATA_DIR, '2dgp_train_features.parquet')\n",
                "TEST_INPUT = os.path.join(DATA_DIR, '2dgp_test_features.parquet')\n",
                "TRAIN_OUTPUT = os.path.join(DATA_DIR, 'train_processed_nn.parquet')\n",
                "TEST_OUTPUT = os.path.join(DATA_DIR, 'test_processed_nn.parquet')\n",
                "SCALER_PATH = os.path.join(DATA_DIR, 'nn_scaler.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "load_data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading TEST data...\n",
                        "Loaded data shape: (7135, 289)\n",
                        "Columns: ['object_id', 'Flux_mean_g', 'Flux_mean_i', 'Flux_mean_r', 'Flux_mean_u', 'Flux_mean_y', 'Flux_mean_z', 'Flux_max_g', 'Flux_max_i', 'Flux_max_r']...\n"
                    ]
                }
            ],
            "source": [
                "# Load data based on mode\n",
                "if TRAIN_MODE:\n",
                "    print(\"Loading TRAINING data...\")\n",
                "    df = pd.read_parquet(TRAIN_INPUT)\n",
                "else:\n",
                "    print(\"Loading TEST data...\")\n",
                "    df = pd.read_parquet(TEST_INPUT)\n",
                "\n",
                "print(f\"Loaded data shape: {df.shape}\")\n",
                "print(f\"Columns: {df.columns.tolist()[:10]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "identify_features",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Metadata columns found: ['object_id']\n",
                        "Number of feature columns: 288\n"
                    ]
                }
            ],
            "source": [
                "# Identify feature columns (exclude metadata)\n",
                "metadata_cols = ['object_id', 'target', 'split', 'SpecType']\n",
                "existing_metadata = [c for c in metadata_cols if c in df.columns]\n",
                "feature_cols = [c for c in df.columns if c not in metadata_cols]\n",
                "\n",
                "print(f\"Metadata columns found: {existing_metadata}\")\n",
                "print(f\"Number of feature columns: {len(feature_cols)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "feature_engineering",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Creating nan_count feature...\n",
                        "nan_count statistics:\n",
                        "count    7135.000000\n",
                        "mean        1.954450\n",
                        "std         5.159255\n",
                        "min         0.000000\n",
                        "25%         0.000000\n",
                        "50%         0.000000\n",
                        "75%         0.000000\n",
                        "max        37.000000\n",
                        "Name: nan_count, dtype: float64\n"
                    ]
                }
            ],
            "source": [
                "# Feature Engineering: Create nan_count column\n",
                "print(\"Creating nan_count feature...\")\n",
                "df['nan_count'] = df[feature_cols].isna().sum(axis=1)\n",
                "print(f\"nan_count statistics:\")\n",
                "print(df['nan_count'].describe())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "imputation",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Imputing NaN values with median...\n",
                        "Total NaN values before imputation: 13945\n",
                        "Total NaN values after imputation: 0\n"
                    ]
                }
            ],
            "source": [
                "# Imputation: Fill NaNs with column median\n",
                "print(\"\\nImputing NaN values with median...\")\n",
                "nan_before = df[feature_cols].isna().sum().sum()\n",
                "print(f\"Total NaN values before imputation: {nan_before}\")\n",
                "\n",
                "# Update feature_cols to include nan_count\n",
                "feature_cols_with_nan_count = feature_cols + ['nan_count']\n",
                "\n",
                "# Fill NaN with median\n",
                "for col in feature_cols:\n",
                "    if df[col].isna().any():\n",
                "        median_val = df[col].median()\n",
                "        df[col] = df[col].fillna(median_val)\n",
                "\n",
                "nan_after = df[feature_cols].isna().sum().sum()\n",
                "print(f\"Total NaN values after imputation: {nan_after}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "scaling",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Loading fitted StandardScaler for TEST data...\n",
                        "Test data transformed using training scaler.\n"
                    ]
                }
            ],
            "source": [
                "# Scaling\n",
                "if TRAIN_MODE:\n",
                "    print(\"\\nFitting StandardScaler on TRAINING data...\")\n",
                "    scaler = StandardScaler()\n",
                "    df[feature_cols_with_nan_count] = scaler.fit_transform(df[feature_cols_with_nan_count])\n",
                "    \n",
                "    # Save the scaler for test data processing\n",
                "    joblib.dump(scaler, SCALER_PATH)\n",
                "    print(f\"Scaler saved to: {SCALER_PATH}\")\n",
                "else:\n",
                "    print(\"\\nLoading fitted StandardScaler for TEST data...\")\n",
                "    scaler = joblib.load(SCALER_PATH)\n",
                "    \n",
                "    if hasattr(scaler, 'feature_names_in_'):\n",
                "        expected_features = scaler.feature_names_in_\n",
                "        \n",
                "        # Check if missing columns\n",
                "        missing = set(expected_features) - set(df.columns)\n",
                "        if missing:\n",
                "            raise ValueError(f\"Test data is missing features expected by scaler: {missing}\")\n",
                "            \n",
                "        # Force the variable to match the scaler's expectation\n",
                "        feature_cols_with_nan_count = list(expected_features)\n",
                "        \n",
                "        # Reorder the DataFrame columns to match the scaler's exact order\n",
                "        X_test_aligned = df[feature_cols_with_nan_count]\n",
                "    else:\n",
                "        # Fallback for older sklearn versions (unlikely needed)\n",
                "        X_test_aligned = df[feature_cols_with_nan_count]\n",
                "        \n",
                "    # Transform using the aligned data\n",
                "    df[feature_cols_with_nan_count] = scaler.transform(X_test_aligned)\n",
                "    \n",
                "    print(\"Test data transformed using training scaler.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "save_output",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Processed data saved to: ..\\data\\processed\\test_processed_nn.parquet\n",
                        "Shape: (7135, 290)\n"
                    ]
                }
            ],
            "source": [
                "# Save output\n",
                "if TRAIN_MODE:\n",
                "    output_path = TRAIN_OUTPUT\n",
                "else:\n",
                "    output_path = TEST_OUTPUT\n",
                "\n",
                "df.to_parquet(output_path, index=False)\n",
                "print(f\"\\nProcessed data saved to: {output_path}\")\n",
                "print(f\"Shape: {df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "verification",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== Verification ===\n",
                        "nan_count column exists: True\n",
                        "Remaining NaN values: 0\n",
                        "\n",
                        "Sample of scaled features (first 5 rows, first 3 feature columns):\n",
                        "   Flux_mean_g  Flux_mean_i  Flux_mean_r\n",
                        "0     2.277641     2.104160     1.374098\n",
                        "1    -0.333957    -0.308295    -0.277532\n",
                        "2     0.302764     1.419263     0.271161\n",
                        "3    -0.230878    -0.249474    -0.203311\n",
                        "4    -0.073605    -0.284840    -0.175776\n"
                    ]
                }
            ],
            "source": [
                "# Verification\n",
                "print(\"\\n=== Verification ===\")\n",
                "print(f\"nan_count column exists: {'nan_count' in df.columns}\")\n",
                "print(f\"Remaining NaN values: {df[feature_cols_with_nan_count].isna().sum().sum()}\")\n",
                "print(f\"\\nSample of scaled features (first 5 rows, first 3 feature columns):\")\n",
                "print(df[feature_cols_with_nan_count[:3]].head())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "MLFinal",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
